# 5장: 할루시네이션을 방지하는 RAG 기반 에이전트

핵심 주제: 학슴 시점에 지식이 고정되어 있고, 할루시네이션을 종종 일으켜 사실적, 개념적 오류가 포함된 답변을 생성하는 LLM의 문제를 극복하기 위해 검색 증강 생성 (RAG; Retrieval-augmented generation) 접근법을 탐구한다.

- 나이브 RAG 탐구하기
- 검색, 최적화, 증강
- 출력에 대한 평가
- ~~RAG와 파인튜닝 비교하기~~
- RAG를 활용한 영화 추천 에이전트 구축하기

## 1. 나이브 RAG 탐구하기

<b>정보 검색</b> (Information retrieval): 미디어에서 필요한 정보를 찾아내는 기술 분야. <u>검색 엔진이 핵심</u> <br>
\- 데이터베이스에 다양한 형태의 문서 모음을 인덱스로 저장 <br>
\- 문서 모음에는 텍스트뿐 아니라 웹페이지, 이미지, 비디오, 코드 조각, 짧은 구절 등도 포함 <br>
\- 각 문서는 저자, 크기, 주제, 키워드와 같은 메타데이터와 연결될 수 있다.

|구분|내용|
|---|---|
|용어|단순히 텍스트로서의 단어. 쿼리에 대한 답을 줄 수 있는 구절.|
|쿼리|사용자가 입력한 용어들로 표현|

\* 쿼리 (Query): 데이터베이스 등에서 원하는 정보를 검색하기 위해 요청하는 것

<p align="center">
  <img src="about:blank"><br>
  <b>Figure 1. 검색 시스템 내 쿼리 처리 원리 [1]</b> 
</p>

- 문서 모음이 <b>인덱싱 과정을 거쳐 정돈된 형태로 데이터베이스에 저장</b>
    - 각 문서에는 <u>메타데이터와 인덱스가 부여</u>됨.
- 사용자의 쿼리는 <b>처리 과정을 거쳐 벡터 표현으로 변환</b>
    - 생성된 벡터는 <u>검색 과정에서 가장 관련성 높은 문서를 찾는데 사용</u>
- 시스템은 최종적으로 관련성 높은 순으로 문서를 보여줌.
- 검색 시스템은 벡터 공간을 기반으로 검색.
    - TF-IDF: <b>정보 검색과 텍스트 마이닝에서 이용하는 <u>가중치</u></b>. 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치
        - 문서 집합을 불러와 TF-IDF 계산
        - 각 문서와 쿼리 간의 점수를 계산하고 이 점수에 따라 순위 결정
        - 벡터 형태의 문서 $d$와 쿼리 $q$간의 점수 (코사인 유사도)는 다음 정의를 따름.

$$
cos(q,d)=\frac{q \cdot d}{|q||d|}
$$

이러한 방식의 검색에는 <u><b>효율적인 저장 구조가 필요</u></b>하다.

<p align="center">
  <img src="https://github.com/user-attachments/assets/3cafbe84-2267-440d-82f2-8299c2d417dd"><br>
  <b>Figure 2. 역인덱스 구조의 예 [2]</b> 
</p>

|구분|원리|
|---|---|
|사전|문서 집합 내에 등장하는 용어들을 나열하고 빈도를 기록|
|포스팅|해당 용어가 어떤 문서에 등장하는지를 가리킴|
- 이런 방식으로 질의에 포함된 용어들이 주어지면, <u>그것들을 포함하는 문서를 <b>효율적으로 찾아내고 유사도를 계산할 수 있다</b></u>.

### 1.1. BM25
<b>개념</b>: 두 개의 파라미터 $b$와 $k$가 추가된 TF-IDF의 변형 <br>
<b>핵심</b>: $b$는 문서 길이 정규화의 중요도를 조절하고, $k$는 단어 빈도 (TF)와 역문서 빈도 (IDF) 사이의 관계를 조절

$$
\rm{BM25}~score=\it{\sum_{i\in q}}~\rm{log}(\it{\frac{N}{df_t}})\frac{t~f_{t,d}}{k(1-b+b(\frac{|d|}{|d_{avg}|}))+t~f_{t,d}}
$$
- $t~f_{t,d}$: 문서 $d$에서 단어 $t$가 나타난 횟수
- $|d|$: 문서 단어 수
- $|d_{avg}|$: 평균 문서 길이

- $k$, $b$: 알고리즘 민감도를 조절하는 하이퍼파라미터
    - 권장 범위
        - $b=0.75,~~1.2<=k<=2.0$
    - $k$를 0으로 설정할 경우, 스코어 계산에서 TF가 사용되지 않음.
        - 문서 내 해당 용어 등장 여부만 고려. 등장 횟수는 반영하지 않음.
    - $k$가 커질수록 TF 가중치는 커진다.
        - $k$는 TF 포화를 조절하는데 쓰인다
        - 단일 쿼리 용어가 단일 문서의 스코어에 얼마나 영향을 미치는지를 컨트롤할 때 $k$가 사용된다.
    - $b=1$은 문서 길이의 정규화를 의미하는 한편, 0은 정규화의 제거를 의미함.
- TF-IDF보다 복잡하지 않기 때문에 대규모 데이터셋으로 확장할 수 있으며 희소 행렬에도 더 강력하게 동작한다.

<B>한계 1</B>: 문서의 의미를 파악하는 것이 아닌 용어의 빈도에 기반한 것이므로 의미적 이해도에 한계 내포. <br>
<B>한계 2</B>: 다의어 맥락을 이해하짐 못함. <br>
<B>한계 3</B>: 어휘의 불일치 문제. 쿼리 내의 용어와 문서 사이에 완전한 중복 (정확한 일치) 이 없는 경우.<br>

#### 1.1.1. BM25 한계에 대한 해결책
<B>핵심</B>: 컨텍스트 정보를 포함하는 밀집 벡터 사용 <br>
\- 토큰의 시퀀스가 주어지면 최종 레이어에서 얻은 표현 $z$를 사용 <br>
\- 단어의 의미를 구분할 수 있는 고차원 표현을 얻을 수 있음. ($z$-score)

\- 편의를 위해 마지막 레이어를 사용한, 여러 개의 레이어 표현을 Average Pooling 하는 것이 권장된다. <br>
\- 이러한 임베딩은 $z$-score와 같은 <u><b>정규화 기법을 적용하는 변환 과정을 거치는 것이 일반적</b></u> → 임베딩 공간의 왜곡을 줄일 수 있음.

- 많은 단어의 벡터는 비등방성 때문에 서로 유사하다.
- 무작위로 선택된 단어들에 있어 코사인 유사도는 생각보다 높다.
- 불량 차원, 즉 컨텍스트 임베딩을 지배하는 소수의 차원 때문인데, 이들은 크기가 크고 불균형적으로 높은 분산을 가지고 있어 문제임.
- 해당 차원들은 구두점 등과 연관성이 높고 실제의미 파악에는 도움되지 않음.
- $z$-score와 같은 정규화 적용 시, <b><u>이런 불필요한 차원의 영향을 줄이고 임베딩을 더 효과적으로 활용</b></u>할 수 있다.

|대표 전략|내용|
|-------|---|
|단일 인코더|- 쿼리와 문서를 모두 모델에 제공함으로써 양방향 셀프 어텐션 메커니즘이 모든 토큰을 참조할 수 있도록 함. <br>- 입력 형식: `[CLS]-query-[SEP]-document`<br>- 이렇게 생성된 `[CLS]` 토큰의 표현은 선형 레이어에 전달되어 유사도 점수를 산출<br>- 해당 레이어는 파인튜닝을 통해 학습됨. <br>- 대부분의 문서는 <u>LLM의 컨텍스트 길이를 초과하기 때문에 <b>서로 중첩되지 않은 청크 단위로 나뉘어 처리</u></b>됨.|
|이중 인코더|- 단일 인코더의 경우 전체 문서 코퍼스와 함께 쿼리를 전달해야 하기 때문에 비용이 많이 든다. <br>- 두 개의 인코더가 독립적으로 동작. <br>- 하나는 쿼리의 표현 `[CLS]q` 추출을 위한 인코더, 다른 하나는 각 문서 또는 청크의 표현 `[CLS]d` 추출을 위한 인코더.<br>- 코퍼스의 각 문서 임베딩을 계산하고 이를 데이터베이스에 저장한다.<br>- 이후 쿼리의 표현과 데이터베이스 내 모든 벡터 사이의 코사인 유사도를 계산한다.<br>- <u>속도는 훨씬 빠르지만 <b>쿼리와 문서 내 용어들 사이에 일부 상호작용이 발생</b>해버리기 때문에 정확도가 떨어짐.</u>|

### 1.2. 할루시네이션 발생 원인
쿼리가 주어졌을 때, LLM은 사시로가 다른 아웃풋을 생성할 수 있는데, 이는 모델이 개념을 설명하는 데는 능숙하지만 특정한 정보를 정확히 유지하는 데 한계가 있다. 

#### 1.2.1. 반복적 패턴 기반 강화
학습 과정에서 개념적 지식은 `반복된 패턴을 통해 강화되어 개념적 측면에서는 우수`하지만, `날짜, 수치, 회귀 정보 같은 특정 사항에는 취약`하다. <BR>
\- 학습 데이터셋에는 정확한 정보뿐 아니라 상충되거나 잘못된 데이터도 포함되어 있다. <bR>
\- 모델이 응답을 생성할 때 분포에서 샘플링하고 학습한 정보를 사용하기 때문에 할루시네이션으로 이어질 수 있다.

#### 1.2.2. 잘못된 아키텍처 설계, 과적합, 학습 과정의 정렬 실패
장문 생성을 목표로 지나치게 최적화되면 모델이 불필요하게 장황해지면서 할루시네이션 위험도 커진다. 마찬가지로, 샘플링 과정에서 `무작위성을 높이는 온도 값을 크게 설정`하면, <B>덜 가능성 있는 토큰이 선택될 확률이 높아져 <U>할루시네이션이 더 자주 발생</U></B>한다. 부적절한 프롬프트 역시 할루시네이션을 부추길 수 있다.

\- 특정 분야에서 모델을 사용할 때 할루시네이션이 더 두드러짐. <BR>
\- 모델이 <B>쿼리를 올바르게 해석하는 데 <u>필요한 컨텍스트를 충분히 갖추지 못했기 때문</u></b>이다. <BR>
\- 모델은 <b>수많은 토큰들에 대해 학습되었으며, <u>특정한 토픽에 대해 국한되지 않았기 때문</u></b>이다. <BR>
\- 손실 (loss)은 텍스트 데이터셋에서 계산되고, 따라서 특정 정보보다는 일반적 지식에 더 많이 적용된다. <br>
\- 모델은 일반적 함수를 더 선호하고 특정 도메인에 적용할 때는 성능이 떨어지는데, 이는 <u>모델 파라미터 수와는 관계없는 공통적인 요소</u>이다.

### 1.3. 할루시네이션 해결 방안
1. LLM 프롬프트의 일부로 컨텍스트를 제공하는 것 (모든 컨텍스트를 프롬프트에 추가하는 것이 가능한 경우) <BR>
\- 연관된 컨텍스트를 다시 찾아야하는 문제가 있다. <BR>
\- 문서가 많을 경우, 복잡하고 소모적일 수 있다.

2. 특정 문서에 대해 추가로 학습시키는 파인튜닝<BR>
\- 연산 비용이 들고 새로운 문서가 추가될 때마다 반복적으로 학습시켜야 하는 부담이 있다.

### 1.4. RAG
**배경**: 2020년 Meta에서 제안. <BR>
**개념**: LLM의 응답 생성 기능을 외부의 데이터베이스 소스로 보강하는 방식. <BR>
**핵심**: 이 데이터베이스는 특정 도메인 전용일 수도 있고 지속적으로 업데이트될 수도 있다. <BR>
\- 쿼리에 잘 응답하기 위해 <B>필요한 문서들을 찾아낸 뒤 <U>LLM의 인-컨텍스트 러닝 능력을 활용해 답변을 생성</U>하는 방식</B>

<p align="center">
  <img src="https://github.com/user-attachments/assets/392f481c-5bfe-4416-ac19-c9103b17dce1"><br>
  <b>Figure 3. RAG 프로세스 개요</b> 
</p>

- 검색을 통해 순위가 매겨진 문서들은 프롬프트에 포함되어 LLM에 제공된다.
- 프롬프트에는 쿼리, 검색된 문서, 추가 정보가 함께 담긴다.
- LLM은 ㅣ 추가 컨텍스트를 활용해 사용자 쿼리에 대한 답변을 생성한다.

|메모리|정의|
|----|----|
|파라메트릭 메모리|LLM에서 얻은 지식|
|외부 or 논파라메트릭 메모리|RAG로부터 얻은 지식|

|구성|내용|
|----|---|
|인덱싱|<b>원시 데이터를 벡터 데이터베이스에 저장하는 과정</b><br>\- 먼저 다양한 형식의 데이터를 수집해 텍스트로 변환 <br>\- 이후 텍스트를 모델의 컨텍스트 길이보다 작은 단위인 청크로 나눈다. <br>\- 각 청크는 임베딩 모델을 통해 벡터 표현으로 변환된다. <br>\- 고유 식별자와 함께 벡터 데이터베이스에 저장된다.|
|검색|<b>사용자의 쿼리가 입력되면, 문서 임베딩에 사용한 동일한 인코더로 벡터화한다.</b> <br>\- 이후 쿼리 벡터와 데이터베이스에 저장된 문서 벡터 간의 유사도 점수를 계산한다. <br>\- 그중 상위 $k$개의 청크를 선택한다.|
|생성|<b>검색된 청크들과 쿼리를 함께 조합하여 LLM에 입력할 프롬프트를 구성</b><br>\- 모델마다 가장 잘 작동하는 프롬프트 방식이 다를 수 있다.<br>\-  작업 목적에 따라 맞춤화된 프롬프트를 설계하기도 한다. <br>\- 필요하다면 이전 대화 기록도 포함시킬 수 있다.|

- 자기회귀 (Autoregressive) 모델의 경우 2장에서 소개한, 주어진 단어들의 문맥을 고려하여 다음 단어 $x$가 나올 확률 $P(w|h)$를 추정하는 방정식으로 정의할 수 있다.

$$P(w|h)=P(w_n|w_{1:n-1})=\prod_{i=1}^nP(w_i|w_{i:i-1})$$

$$p(x_1, x_2, \cdots, x_n)=\prod_{i=1}^np(x_i|x_{<i})$$

질의응답 작업의 경우 `질문` 또는 `쿼리`가 주어지면 다음과 같이 수정 할 수 있다.

$$p(x_1, x_2, \cdots, x_n)=\prod_{i=1}^np(q;x_{<i})$$
여기에 RAG를 적용하면 추가 요소들이 포함된다. 즉, 프롬프트 $Pr$, 검색된 컨텍스트 $R$, 쿼리 $q$가 모두 결합되어 확률 계산에 반영된다.

$$p(x_1, x_2, \cdots, x_n)=\prod_{i=1}^np(x_i|Pr;~R;~q;~x_{<i})$$
<p align="center">
  <img src="https://github.com/user-attachments/assets/529171c1-a41d-4986-927a-c96d26481e29"><br>
  <b>Figure 4. RAG 프로세스 단계 시각화 [3]</b> 
</p>

### 1.5. 스팬 추출
**개념**: RAG 아키텍처의 대안. 가장 적절한 청크를 검색하는 대신, 언어 모델 (대체로 BERT 파생 모델)을 활용해 쿼리에 대한 답이 들어 있는 구절 (Span) 을 직접 찾아낸다. (Span labeling) <BR>
\- 코퍼스가 위키백과이고 쿼리가 `프랑스의 대통령은 누구인가?`라면, 해당 질문에 대한 답을 포함한 페이지의 특정 구절을 표시해 반환한다. <BR>
\- 반면, RAG에서는 질문과 관련되 텍스트 청크를 검색한다.

```
RAG와 스팬 추출, 두 방식 모두 할루시네이션을 줄이고 오픈 도메인 질의응답의 성능을 끌어올리는 유력한 방법이다.
```

## 2. 검색, 최적화, 증강
### 2.1. 청크 분할 전략
<U>텍스트를 벡터 데이터베이스에 임베딩하기 전에는 <B>반드시 청크 단위로 나눠야한다</B></U>. 이 작업은 어떤 정보가 벡터에 담기고, 검색 단계에서 어떤 정보가 노출되는지를 좌우하므로 매우 중요한 요소이다. <U>청크가 지나치게 작으면 컨텍스트를 잃고</U>, 반대로 <U>너무 크면 불필요한 정보까지 포함되어 응답의 정확성이 떨어질 수 있다.</U>

이는 <U><B>쿼리별 정보의 검색에 영향을 미친다</B></U>. 청크 크기가 클수록 프롬프트에 도입되는 토큰의 양이 많아져 추론 비용이 증가한다. <U>과도한 컨텍스트 또한 할루시네이션을 유발할 수 있고 LLM의 성능을 저하시킬 수 있다</U>. 또한 <U><B>청크 크기는 임베더의 컨텍스트 길이를 초과해서는 안 되며</B>, 그렇지 않으면 정보를 잃게 된다</U>. 다시 말해 `청크의 크기`는 `검색`과 `생성`의 품질 모두에 영향을 미치는 `중요한 요소`이다.

|분할 전략|원리|특징|
|-------|----|----|
|문자 단위 청크 분할|문서를 미리 정해진 문자 수나 토큰 수를 기준으로 나누는 방식|단순한 구연, 저렴한 비용|
|가변적인 무작위 청크 사이즈|고정된 문자 단위 청크 분할 방식에서 변형된 방식|컬렉션이 비균질적이고 더 의미론적인 맥락을 포착할 수 있을 때 사용|
|컨텍스트 인식 청크 분할|정규 표현식을 이용해 텍스트를 청크로 나누는 전략|\- 마침표, 쉼표, 단락 구분 등을 기준으로 분할 할 수 있다.<BR>\- 복문 (compound setence)이나 정제되지 않은 텍스트에서는 경계가 불명확해 청크 크기가 일정하지 않을 수 있다.|
|재귀적 청크 분할|컨텍스트 인식 방식으로 텍스트를 나눈 뒤, 생성된 청크를 미리 정한 토큰 수에 맞춰 다시 합친다.|관련 있는 정보를 같은 청크에 묶어 의미적 일관성을 높일 수 있다.|
|계층적 클러스터링|텍스트의 구조적 위계를 존중하면서 분할하는 방식<BR>\- 섹션-하위 섹션-문단-문장 등 계층 구조에 따라 나눈다.|\- 복잡하고 구조가 명확히 정의된 문서에 특히 유용하다.<BR>\- 도출된 구조를 분석하면 청크 간의 관계를 더 잘 이해할 수 있다.<BR>\- 문서의 형식이 불완전하거나 구조가 모호한 경우에는 제대로 작동하지 않는 한계가 있다.|
|시멘틱 청크 분할|단어의 맥락과 의미를 고려하는 방식|텍스트 안에서 물리적으로 멀리 떨어져 있떠라도 의미적으로 가까운 문장들을 한데 묶는다.<br>\- 대표적으로 `k-평균 청크 분할`|
|명제 기반 청크 분할|문장을 기준으로 나누는 대신 문맥적 이해에 따라 나누는 방식|\- `명제`는 `'프랑스의 수도는 파리다'`처럼 사실을 담은 최소 표현을 의미함.<br>\- 이러한 명제들은 LLM을 사용해 의미적 응집성 기준으로 그룹화된다.<BR>\- 결과 품질은 우수할 수 있으나 연산 비용이 크고 사용한 LLM의 성능과 설정에 민감하다.|
|통계적 병합|문장의 임베딩에서 유사도와 차이를 평가하여 그것들을 합칠지 나눌지 결정하는 방식|임베딩 후 통계적 속성의 차이를 평가하고 미리 정의된 임계값을 초과하면 문장을 분리하는 방식|
|멀티모달 청크|-|텍스트와 이미지가 함께 포함되어있는 PDF 파일의 경우, 청크 분할 파이프라인에서 텍스트와 이미지를 모두 추출할 수 있어야함.|

- 모든 경우에 어울리는 `'최고의'` 청크 분할 방식은 존재하지 않는다.
- 최선의 방법은 특정 상황과 데이터에 가장 잘 맞는 방식이다.
- 아래는 청크 분할 방식 채택을 위한 지침 수립

1. <B>문서 구조에 따른 청크 분할</B>: 이질적인 문서 모음이라면 파일 형식별 파이프라인을 따로 두는 편이 좋다.
2. <B>성능과 자원 최적화</B>: 저장 공간이나 연산 비용에 제약이 있따면 단순한 고정 크기 청크 분할이 최적일 수 있다. 다만, 이 방식은 텍스트에 대한 사전 지식이 필요하므로 일반 사용자가 쓰는 시스템에는 최적이 아닐 수 있다.
3. <B>모델 컨텍스트 제한 고려</B>: 청크 크기는 반드시 임베딩 모델과 최종적으로 사용할 LLM의 컨텍스트 길이를 모두 만족해야 한다.
4. <B>사용자 질의 패턴에 맞춘 전략</B>: 사용자가 어떤 질문을 던지는지에 맞춰 전략을 조정한다. 여러 사실을 찾아야 하는 경우라면 작은 청크에 직접적인 답을 담는 것이 유리하다.

결론적으로, 개발자는 RAG 시스템에 맞춰 여러 청크 분할 전략을 테스트하고, 텍스트와 출력 결과를 면밀히 검토해야 한다. 어떤 전략을 택하든 평가 절차를 통해 품질을 수치로 확인하는 과정이 반드시 뒤따라야 한다.

### 2.2. 임베딩 전략
임베딩은 텍스트를 고차원 공간에 존재하는 밀집 벡터 표현으로 나타낸 것이다. 이렇게 얻은 벡터를 활용해 쿼리에 적합한 컨텍스트를 찾는다. 임베딩은 회소 벡터를 생성하는 인코더 (TF-IDF, BM25 등)나 밀집 벡터를 생성하는 인코더를 통해 얻을 수 있다.

밀집 벡터를 생성하는 인코더는 주로 <b>트랜스포머 기반 모델</b>이며, 학습 가능하다는 장점이 있어 쿼리와 청크 간의 유사도 작업에 적합하도록 조정할 수 있다.

가장 널리 쓰이는 방식은 <b><u>BERT 기반 이중 인코더</b></u>다. 이 방식은 쿼리 인코더와 문서 인코더라는 두 개의 BERT 인코더를 사용한다. 초기 RAG 모델에서는 두 인코더의 가중치가 동일하게 고정되어 있었고, 단일 레이어만 학습하여 임베딩 벡터를 생성했다. <U>동일한 가중치를 사용</U>하기 때문에, 쿼리를 입력한 후 문서를 입력하고 유사도를 계산하는 방식으로 동작했다. 반면 최신 모델들은 <u><b>가중치를 파인튜닝하여 더 정교한 임베딩 벡터를 생성</b>함으로써 성능을 높인다.</u>

또 다른 방법으로는 처음부터 모델을 학습시키는 방식이 있다. 하지만 일반적으로 비지도 학습으로 훈련된 LLM을 가져와 임베딩과 검색에 맞게 적응시키는 것이 더 효과적이다. 보통 이 모델은 <b>대조 학습(contrastive learning)</b>을 사용한다. 대조 학습은 임베딩의 형태로 <b>의미적 표현(semantic representations)</b>을 학습하는 데 사용되는 기법이다. 3장에서는 이미지와 캡션을 사용해 학습시킨 CLIP을 사용했다. 이번에는 쿼리와 가장 유사한 문서를 찾을 수 있는 임베딩을 생성하도록 모델을 훈련시키고자 한다. 이때 가장 널리 사용되는 데이터셋 중 하나는 다중 장르 자연어 추론 (MultiNLI, Multi-Genre Natural Language Inference) 코퍼스로, 약 43만 3천 개의 문장 쌍이 포함되어 있으며, 두 텍스트 간의 관계가 <b>함의(entailment), 모순(contradiction), 중립(neutral)</b>으로 주석 처리되어 있다.

대조 학습에서는 <b>긍정 예시(positive example)</b>와 <b>부정 예시(negative example)</b>가 필요하다. 하나의 문장을 기준으로 삼았을 때, 그 문장의 임베딩은 긍정 예시와는 최대한 가깝게, 부정 예시와는 최대한 멀어지도록 학습한다. 이때 MultiNLI 데이터셋에서는 함의 관계에 해당하는 문장이 긍정 예시가 되고, 모순 관계에 해당하는 문장이 부정 예시가 된다.

데이터셋이 준비되면 임베딩 모델은 해당 작업에 적합한 손실 함수를 사용해 학습된다. 대표적인 손실 함수는 다음과 같다.

- <b>코사인 유사도 손실</b>: <u>두 문장의 의미적 유사성을 측정하는 가장 대표적인 방법</u>이다. 원본 문장과 긍정 예시 문장은 유사도가 1에 가깝도록, 원본 문장과 부정 예시는 0에 가깝도록 학습이 진행된다. <u>손실 계산은 두 문장 간 유사도를 구한 뒤, 이를 예측된 레이블(label)과 비교하는 방식</u>으로 이루어진다.

- <b>다중 부정 랭킹 손실</b>: InfoNCE라고도 불림. <u>원본 문장과 그에 해당하는 긍정 예시(함의 문장)만 있고 부정 예시는 따로 주어지지 않는다</u>. 문장과 관련 문장(긍정 예시) 간의 유사성을 극대화하면서, 관련 없는 예시(부정 예시)와의 유사성은 최소화하는 것이 핵심이다. 이렇게 하면 이 작업은 분류 작업이 되며 크로스 엔트로피를 사용할 수 있다. 그러나 이때 <u>부정 예시는 완전히 무관한 경우가 많아 이 작업은 모델에게 너무 쉬울 수 있다</u>(<b>대신 관련성이 있으나 정답이 아닌 부정 문장을 추가하는 것이 더 좋다</b>).

임베딩 모델의 선택은 시스템 성능에 결정적인 영향을 미친다. 성능이 낮은 임베딩 모델을 사용하면 검색 성능이 저하되고 질의와 관련 없는 컨텍스트가 제공될 수 있다. 이는 오히려 할루시네이션 발생 위험을 높일 수 있다. 따라서 인코더를 선택할 때는 다음 요소들을 반드시 고려해야 한다.

|구분|내용|
|-|-|
|비용|임베딩 모델은 트랜스포머 기반이므로 크기가 클수록 연산 비용이 증가한다. 폐쇄형 모델은 API 호출량에 따라 비용이 발생하며 문서를 임베딩하는 비용뿐만 아니라 질의마다 추가 연산 비용이 발생한다.|
|저장 비용|임베딩 벡터의 차원이 클수록 이를 저장하는 데 필요한 자원도 늘어난다.|
|지연 시간|모델 크기가 커질수록 질의응답 속도가 느려진다.|
|성능|성능을 최우선으로 한다면 높은 비용을 감수할 가치가 있다. 일반적으로 모델이 클수록 성능도 더 좋다.|
|도메인 요구사항|대부분은 영어에 최적화되어 있지만, 일부 모델은 최대 100개 언어까지 지원한다. 긴 텍스트를 다뤄야하는 도메인의 경우 이에 특화된 모델이 필요할 수 있다.|

어떤 인코더 모델을 선택할지는 간단하지 않으며 여러 요인을 함께 고려해야 한다. 좋은 출발점은 Hugging Face의 MTEB 리더보드 [[**Link**](https://huggingface.co/spaces/mteb/leaderboard)] 참고하는 것이다. 이 리더보드를 보면 다양한 벤치마크와 과제에서 최신 인코딩 모델들의 성능을 확인할 수 있다. 다만, 대부분의 결과는 모델 개발자가 자체 보고(self-reported)한 값이며, 학습 데이터에 벤치마크 데이터가 포함되었을 수 있어 성능이 과대평가될 가능성이 있다. 따라서 하나의 모델만 선택할 것이 아니라 여러 모델을 실제 데이터셋에서 테스트해보는 것이 바람직하다. 그럼에도 리더보드는 선택에 도움이 되는 다음과 같은 주요 정보를 제공한다.

|주요 정보|내용|
|-|-|
|검색 평균|여러 데이터셋에서 정규화된 할인 누적 이득 (NDCG, normalized discounted cumulative gain)을 계산한 값으로, 검색 시스템 순위를 매기는 데 사용하는 평가 지표이다.|
|모델 크기|연산 비용과 필요한 자원 규모를 가늠할 수 있다.|
|최대 토큰 수|컨텍스트에 활용할 수 있는 최대 토큰 길이를 의미한다.|
|임베딩 차원|생성된 임베딩 벡터의 크기를 나타낸다.|

또한 리더보드가 측정하는 성능은 대부분 일반 도메인 기준이라는 점도 유념해야 한다. 따라서 특정 도메인이나 과제에 대해서는 기대보다 성능이 낮을 수 있다.

인코더를 선택한 뒤에는 성능에 영향을 주지 않으면서 비용을 줄이는 방법도 고려할 수 있다. 벡터 임베딩의 각 차원은 보통 float 형식으로 저장되며, 이 경우 차원당 4바이트의 메모리를 차지한다. 대규모 데이터에서는 이 저장 비용이 급격히 커질 수 있다. 이때 3장에서 논의했던 양자화 (quantization) 기법을 임베딩 모델에도 적용할 수 있다. 특히 이진 양자화는 모델을 차원당 1비트로 줄여 메모리와 저장 공간을 최대 32배까지 절감할 수 있다. 가장 간단한 이진 양자화는 0을 임곗값 기준으로 사용하는 방식이다.

$$f(x) = \begin{cases} 0 & \text{if } x \le 0 \\ 1 & \text{if } x > 0 \end{cases}$$

양자화 후에는 해밍 거리 (Hamming distance)를 활용하면 벡터를 더 효율적으로 비교할 수 있다. 해밍 거리는 이진 벡터 간 차이를 측정하는 데 가장 적합한 지표이다(자세한 내용은 이 장 끝의 ‘더 읽을거리’에 있는 링크에서 확인할 수 있다). 이진 인코딩은 메모리 절감과 속도 향상 면에서 큰 장점을 제공하지만, 일반적으로 성능 저하를 동반한다. 다만, 더 정교한 버전들은 검색된 청크를 재점수화 (re-scoring)하는 방식으로 최대 96%의 유사도를 유지할 수 있다. 이러한 양자화는 다소 극단적인 방식으로 간주되며, 보통은 float32 형식을 int8 형식으로 변환하는 것이 합리적인 절충안이다 (int8은 256개의 구별된 값으로 수치를 표현한다). 앞서 설명했듯이, 이 변환 과정은 벡터를 재보정하여 이루어진다.

또 다른 방법으로 마트료시카 표현 학습 (Matryoshka Representation Learning)이 있다. 일반적으로 딥러닝 모델은 정보를 전체 벡터 차원에 고르게 분산시키는 경향이 있는데, 이 기법은 정보를 점진적으로 축소된 차원 공간에 압축하려는 접근이다. 즉, 임베딩 벡터의 차원을 줄이면서도 성능 손실을 최소화하는 방식이다. 마트료시카 임베딩에서는 작은 크기의 임베딩을 생성하지만 이를 더 큰 임베딩처럼 활용할 수 있다. 그 이유는 모델이 가장 중요한 정보를 앞쪽 차원에, 덜 중요한 정보를 뒤쪽 차원에 저장하도록 학습되기 때문이다. 따라서 후속 작업에서 벡터의 일부 차원을 잘라내더라도 품질이 유지된다.

학습 과정에서는 먼저 텍스트 배치 (batch)에 대한 임베딩을 생성하고 손실을 계산한다. 마트료시카 임베딩 모델에서는 서로 다른 차원에서의 임베딩 품질도 함께 고려해 손실을 합산한다. 따라서 모델은 가장 중요한 정보가 앞쪽 차원에 위치하도록 가중치를 최적화하게 된다.

벡터를 생성한 뒤에는 이를 저장해야 한다.

### 2.3. 임베딩 데이터베이스
벡터 데이터베이스는 고차원 벡터를 저장하는 데 특화된 데이터베이스이다. 텍스트와 같은 비정형 이터나 반정형 데이터를 효율적으로 다룰 수 있도록 최적화되어 있다. 주요 기능은 벡터를 효율적으로 저장하고 인덱싱하며 검색하는 것이다.

`최고의` 벡터 데이터베이스는 없지만, 특정 프로젝트에 가장 적합한 데이터베이스는 분명히 존재한다. 선택 시 고려해야 할 대표적인 기준은 다음과 같다.

|기준|내용|
|-|-|
|오픈 소스 vs. 상용 소스|\- 오픈소스는 투명성이 높고 자유로운 커스터마이징이 가능하며 비용이 들지 않고 커뮤니티 지원이 활발하다. <br>\- 상용 소스는 비용이 많이 들 수 있지만 전담 기술 지원을 받을 수 있다. (라이센스 조건이 프로젝트와 호환되는지 확인 필요)|
|언어 지원|모든 데이터베이스가 모든 라이브러리와 호환되는 것은 아니므로, 반드시 프로젝트와 통합이 가능한지 확인해야 한다.|
|성숙도|특히 상용 프로젝트라면, 시스템이 안정적이고 확장 가능하며 신뢰할 수 있는지가 중요하다.|
|성능|\- 삽입 속도: 새로운 벡터를 데이터베이스에 추가하는 속도로 지연 시간에 영향을 준다. <br>\- 질의에 대한 응답으로 벡터를 찾는 데 걸리는 시간이며 지연 시간에 직접적인 영향을 미친다. <br><br> 성능을 높이기 위해 인덱스 구조나 캐싱 시스템, 특정 알고리즘 등을 활용한다. 이 때 인덱스 구조는 검색 속도를 향상하도록 설계하고, 캐싱 시스템은 자주 사용하는 데이터를 따로 저장해 빠르게 제공한다.|
|구성 요소 통합|시스템은 LLM과 임베딩 모델 외에도 다양한 구성 요소를 포함할 수 있따. 데이터베이스가 인코더 및 관련 라이브러리와 원활히 통합될 수 있는지 확인해야한다.|
|비용|클라우드 기반 솔루션은 비용이 높을 수 있으므로 사전에 예산을 설정하는 것이 바람직하다. 시스템의 안정적인 운영을 위한 유지보수와 기술 지원 비용도 고려해야 한다.|

일부 벡터 라이브러리는 정적 (static)으로 설계되어 인덱스 데이터가 불변 (immutable)이다. 이러한 경우 새로운 데이터를 추가하거나 갱신하기가 어렵다. 대표적으로 <B>FAISS(Facebook AI Similarity Search) </B>와 같은 라이브러리는 CRUD (생성, 읽기, 갱신, 삭제) 연산을 제대로 지원하지 않기 때문에, 여러 사용자가 동시에 접속해 데이터를 다뤄야 하는 동적 시스템에는 적합하지 않다. 하지만 데이터가 고정되어 있고 단순히 읽기 전용 접근만 필요하다면 FAISS는 좋은 선택이 될 수 있다.

SQL 데이터베이스 중에는 벡터를 지원하는 것도 있다. 이러한 데이터베이스는 연관된 메타데이터를 효율적으로 인덱싱하도록 한다. 그러나 일반적으로 확장성이 떨어지고 벡터 차원 수에 제한이 있으며 성능도 전용 솔루션보다 낮은 편이다. 따라서 SQL 기반 시스템과의 통합이 필요한 내부 프로젝트에는 적합하지만, 대규모 확장성과 성능이 중요한 경우에는 적합하지 않다.

반면 벡터 전용 데이터베이스는 성능 면에서 가장 우수하다. 이런 데이터베이스는 벡터 검색과 인덱싱을 위해 최적화된 알고리즘을 구현하고 있으며, 특히 다수의 시스템이 근사 최근접 이웃 (ANN, approximate nearest neighbors) 기법을 변형해 사용한다. ANN은 효율성, 저장 공간, 정확도 간의 균형을 잘 맞춰준다. 예를 들어 <B>HNSW (hierarchical navigable small world) </B>는 약간의 정확도를 희생하는 대신, Flat 인덱싱 같은 정밀 알고리즘보다 훨씬 빠른 검색 속도를 제공한다. 이러한 벡터 전용 데이터베이스는 LlamaIndex, LangChain 같은 최신 프레임워크와도 잘 통합되며 실무에서 널리 활용하고 있다.

이제 벡터를 데이터베이스에 저장했다면, 다음 단계는 시스템이 실제로 얼마나 잘 동작하는지 평가하는 것이다.

## 3. 출력에 대해 평가하기


## 4. RAG를 활용한 영화 추천 에이전트 구축하기
|파일|링크|비고|
|---|---|---|
|file.ipynb|[[**Link**]()]||
|file.ipynb|[[**Link**]()]||


## Reference
[1] <br>
[2] 역색인(Inverted indexing), Naver blog, [Online] https://blog.naver.com/sjc02183/221783125922 (Accessed 05 Feb 2026)<br>
[3] Gao, Yunfan, et al. "Retrieval-augmented generation for large language models: A survey." arXiv preprint arXiv:2312.10997 2.1 (2023).<br>
[4] <br>
[5] <br>
[6] <br>
[7] <br>
[8] <br>

