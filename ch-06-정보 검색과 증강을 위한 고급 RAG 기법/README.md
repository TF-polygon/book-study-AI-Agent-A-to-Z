# 6장: 정보 검색과 증강을 위한 고급 RAG 기법

핵심 주제: 고급 RAG에서 파이프라인의 각 단계를 수정하거나 개선하는 방법을 학습하고, 이를 통해 살펴본 나이브 RAG의 한계를 해결하고 전체 프로세스를 더 정밀하게 제어. 더 높은 유연성에 대한 요구가 어떻게 모듈형 RAG라는 발전 단계로 이루어졌는지도 다룬다. <br>
\- RAG를 실제 제품으로 구현할 때 고려해야 할 중요한 요소 논의
- 나이브 RAG의 문제점
- 고급 RAG 파이프라인 살펴보기
- 모듈형 RAG와 다른 시스템 통합하기
- 고급 RAG 파이프라인 구현하기
- RAG의 확장성과 성능 이해하기
- ~~미해결 과제와 미래 전망~~

## 1. 나이브 RAG의 문제점
의료, 법률, 금융과 같은 산업용 애플리케이션에서는 나이브 RAG만으로는 충분하지 않으며 보다 정교한 파이프라인이 필요하다.

|구분|내용|
|--|--|
|검색 단계의 문제|- 정밀도 (검색된 청크가 부정확함)와 재현율 (모든 관련 청크를 빠짐없이 찾는 것) 측면에 어려움 존재<br>- 지식 베이스가 오래되었을 수 있다.<br>- 할루시네이션이 발생할 수 있다.<br>-|
|최상위 문서 누락|쿼리에 답하는 데 필수적인 문서가 목록 상단에 위치하지 않을 수 있다.|
|컨텍스트 내 관련 정보 부족|정답이 들어 있는 문서를 찾았더라도 LLM의 컨텍스트 길이 제약 때문에 모두 싣지 못할 수 있다.|
|추출 실패|올바른 컨텍스트를 LLM에 전달했음에도 정답을 뽑지 못할 수 있다.|
|잘못된 형식의 답변|쿼리에 추가적인 형식 요구가 있을 수 있다.|
|부적절한 구체성|생성된 답변이 충분히 구체적이지 않거나 지나치게 구체적일 수 있다.|
|증강 장애 또는 정보 중복|데이터베이스에 서로 다른 코퍼스에서 수집된 문서가 뒤섞여 있고 많은 문서가 중복된 정보를 담거나 스타일과 톤이 제각각일 수 있다.|
|불완전한 답변|틀리지는 않았지만 중요한 정보가 빠진 불완전한 답변이 생성될 수 있다|
|유연성 부족|- 시스템 자체가 유연하지 않아 효율적인 업데이트가 어렵고 사용자 피드백이나 이전 상호작용을 반영하지 못하는 경우<br>- 시스템이 코퍼스에 자주 등장하는 특정 파일 포맷을 지원하지 못하는 경우|
|확장성과 성능 문제|- 임베딩 수행, 응답 생성 등 전반적인 처리 속도가 느릴 수 있다.<br>- 초당 여러 문서를 처리해야 하는 환경에서 시스템이 이를 감당하지 못할 수도 있다.<br>- 제품이나 도메인 특유의 성능 문제도 있을 수 있으며, 민감한 데이터를 다루는 경우 보안 또한 큰 과제가 된다.|

## 2. 고급 RAG 파이프라인 살펴보기
<b>개념</b>: 나이브 RAG에서 드러난 한계를 해결하기 위해 여러 개선 사항을 도입한 형태. (RAG 패러다임 최적화를 위한 세밀한 구성 요소 조정) <br>
<b>핵심</b>: RAG 파이프라인의 각 단계인 검색 전과 검색 후 단계에서 이루어짐

### 나이브 RAG의 문제점 한꺼번에 정리
|문제점|해결책|
|--|--|
|대규모 혹은 장문의 문서로 인한 지연 및 성능 저하|<b>계층적 인덱싱 활용</b>|
|코퍼스에 고유한 구조가 있을 때, 평면적 계층 구조가 검색의 관련성을 제한하는 문제|<b>계층적 인덱싱 적용 </b>|
|낮은 검색 정확도와 특정 도메인을 일반화하는 데 따르는 어려움|<b>가상 질문과 HyDE 사용 </b>|
|세분화된 청크 분할 시 컨텍스트 손실|<b>컨텍스트 강화 활용</b>|
|복잡한 쿼리 및 초기 검색에서 낮은 재현율|<b><U>쿼리 변환 적용</b></U>|
|특정 용어나 키워드에 대한 컨텍스트 불일치|<b><U>하이브리드 검색 활용</b></U>|
|다양한 유형의 쿼리를 비효율적으로 처리하는 문제|<b><U>쿼리 라우팅 구현</b></U>|
|임의의 상위 k개 차단으로 인한 관련 청크 손실|<b><U>재순위화 적용</b></U>|
|LLM 컨텍스트 내 정보 손실 또는 비효율성|<b>컨텍스트 압축 적용</b>|
|비효율적인 응답 생성|<b>응답 최적화</b>|
|대화형 시스템에서의 메모리 한계|<b>챗엔진 기법 활용</b>|
|복잡한 추론 또는 동적 쿼리 적응의 필요성|<b>적응형 및 다단계 검색 채택</b>|
|생성된 응답에서 출처 추적 부재|<b>인용 포함</b>|
|쿼리 복잡성 또는 모달리티를 기반으로 한 파이프라인 맞춤화 필요성|<b>RAG 파이프라인 확장</b>|

### 2.1.1. 쿼리 변환
**개념**: LLM을 활용해 검색 성능을 개선하는 기법
- 쿼리가 너무 복잡한 경우 이를 겨러 개의 쿼리로 분해할 수 있다.

- 실제로 원래 쿼리에 직접 대응하는 청크는 없더라도 분해된 하위 쿼리에 대응하는 청크는 더 쉽게 찾을 수 있다.

- 예를 들어, `전신과 전화기의 발명가는 누구인가`라는 질문은 두 개의 독립된 쿼리로 분해하는 것이 바람직하다.

|기법|내용|
|--|--|
|스텝백 프롬프팅|\- LLM을 사용해 상위 수준의 컨텍스트와 일치하도록 더 일반적인 쿼리를 생성<BR>\- 이는 인간이 어려운 문제에 직면했을 때 한 발짝 물러서서 추상화를 통해 고차원 원리를 파악하는 사고 방식에서 비롯됨.<BR>\- 이 때 생성된 상위 쿼리와 원래 쿼리를 각각 임베딩하고, 두 결과를 함께 LLM에 제공해 답변을 생성한다.|
|쿼리 재작성|\- 초기 쿼리를 검색에 더 적합하도록 LLM을 재구성하는 방식|
|쿼리 확장|\- 쿼리에 용어를 추가하여 원래 쿼리와 어휘적으로 겹치지 않는 문서도 검색할 수 있도록 하는 방식<BR>\- 검색 재현율을 개선할 수 있다.|
- 쿼리 확장에서도 LLM을 활용해 쿼리를 변형할 수 있다.
    - LLM에게 쿼리에 대한 답변을 생성하게 한 뒤, 그 답변과 쿼리를 함께 임베딩해 검색에 활용
    - 원래 쿼리와 비슷한 여러 개의 쿼리를 생성한다. 식별자는 숫자 $n$ (ex: Query $n$)
    - 이렇게 만든 $n$개의 쿼리 집합을 벡터화한 뒤 검색에 사용한다.

### 2.1.2. 키워드 기반 검색과 하이브리드 검색
**키워드 기반 검색의 개념**: 특정 키워드를 정확히 일치시켜 검색하는 방식 <br>
\- 제품명, 회사명, 특정 업계 전문 용어와 같은 특정 용어 검색에 유리하다. <br>
\- 오탈자나 동의어에 취약하고 문맥을 파악하지 못한다는 한계가 있다.
- 벡터 기반 또는 시맨틱 검색은 쿼리의 의미를 포착할 수 있지만 특정 키워드나 용어를 정확히 찾아내지는 못한다.
- 특히 마케팅과 같은 일부 도메인에서는 키워드가 일치가 반드시 필요한 경우가 있다.

**하이브리드 검색의 개념**: 키워드 검색과 벡터 검색을 결합해 두 방시의 장점을 모두 취하는 방법이다. <br>
\- 대표적 예: BM25. 희소 임베딩을 생성함

BM25를 사용하면 쿼리에 있는 특정 용어를 포함하는 문서를 식별할 수 있다. 따라서 하이브리드 검색에서는 두 가지 임베딩을 생성한다.
1. BM25 기반 희소 임베딩
2. 트랜스포머 기반의 밀집 임베딩

최적의 청크를 선택하려면 일반적으로 두 검색 방식이 미치는 영향을 균형 있게 조정해야 한다. 최종 점수는 두 점수의 가중 조합이다.

$$
\rm{score_{hybrid}= (1-\alpha) \cdot score_{sparse} + \alpha \cdot socre_{dense}}
$$
- $0 <= \alpha <= 1$
- 일반적으로 $\alpha$는 0.4 또는 0.5 정도, 일부 연구에서는 0.3을 제안하기도 함.

**핵심**: BM25와 같은 순수 키워드 기반 검색은 모호하거나 서술적인 쿼리에는 약하고, 반대로 벡터 기반 시맨틱 검색은 특징 제품명과 같은 정확한 매칭을 놓칠 수 있다. <U><B>하이브리드 검색은 이 두 가지를 결합해 해결한다</B>.</U>

### 2.1.3. 쿼리 라우팅
시스템은 서로 다른 유형의 데이터베이스, 서로 다른 데이터 소스 또는 다양한 모달리티와 상호작용해야 할 수 있다. 일부 쿼리는 RAG를 사용하지 않아도 되고 모델의 파라메트릭 메모리만으로도 충분할 수 있다.

<B>쿼리 라우팅 </B>은  이러한 경우 시스템이 쿼리에 어떻게 응답할지 제어하는 방식 <BR>
\- `if-else` 구문과 비슷하지만, 보통 LLM을 라우터로 사용해 매 쿼리마다 결정을 내린다. <BR>
\- 항상 올바른 결정을 내리지는 않지만 성능에 긍정적인 영향을 줄 수 있다.

|구분|내용|
|---|---|
|논리적 라우터|`if-else` 조건문으로 구성된 논리 규칙|
|키워드 라우터|쿼리와 옵션 목록의 키워드를 매칭해 경로를 선택하는 방식|
|제로샷 분류 라우터|입력된 쿼리에 대해 LLM은 제공된 레이블 목록 중에서 해당 쿼리에 적합한 경로 레이블을 할당한다.|
|LLM 함수 호출 라우터|다양한 경로가 특정 설명과 함께 함수 형태로 정의되며, 모델은 이들 함수 중 하나를 선택함으로써 쿼리를 어느 경로로 보낼지 결정한다. LLM의 의사결정 능력을 활용하는 방식.|
|의미 기반 라우터|시맨틱 검색을 활용하여 최적의 경로를 결정한다.|

- 컨텍스트를 찾은 뒤에는 이를 쿼리와 통합해 LLM에 제공해 생성을 진행해야 함.
- 이 프로세스를 개선하기 위한 여러 <U>검색 후 전략</U>이 존재한다.

### 2.1.4. 재순위화
**개념**: 문서 검색 단계에서 최대한 많은 결과를 확보해 검색 재현율을 높이면서 동시에 LLM에 제공하는 문서 수를 최소화 해 LLM의 재현율을 극대화하는 것

1. 먼저 일반적인 검색을 수행해 많은 수의 청크를 찾는다.
2. 이후 재순위화 모델을 사용해 청크의 순서를 다시 정렬하고, 상위 $k$개의 청크만을 선택해 LLM에 제공한다.

재순위화는 LLM에 제공되는 청크의 품질을 향상시키고 시스템의 할루시네이션을 줄여 준다. 또한 재순위화는 쿼리와 관련된 상반된 정보까지 고려하여 쿼리와 맥락적으로 가장 적합한 청크를 선별한다.

|접근 방식|내용|
|----|---|
|크로스 인코더|트랜스포머 모델 두 개의 텍스트 시퀀스를 입력으로 받아 0과 1 사이의 유사도 점수를 반환|
|멀티 벡터 재순위화 모델|ColBERT와 같은 트랜스포머 모델로 크로스 인코더보다 계산량이 작으며, 두 시퀀스를 입력받아 0과 1 사이의 유사도 점수를 반환|
|LLM 기반 재순위화|LLM 자체를 재순위화 모델로 활용할 수 있으며 LLM의 순위 매기기 능력을 향상시키기 위해 여러 가지 전략이 사용된다<BR><BR>\- **포인트와이즈 방식**: 쿼리와 단일 문서의 관련성을 평가하는 방식<BR>\- **페어와이즈 방식**: 쿼리와 두 개 문서를 함께 제시하고 LLM에게 더 관련 있는 문서를 선택하도록 요청한다|
|파인튜닝된 LLM|순위 매기기 작업에 특화된 모델|

- 일반적으로 멀티 벡터 모델은 연산 비용이 낮지만 제한적인 성능을 보이고 LLM 기반 방식은 성능은 가장 좋지만 연산 비용이 크다

- **대안#1 후처리 기법**
    - 유사도가 특정 $\cdot$ 점수 임곗값 이하이거나 특정 키워드가 포함되지 않은 경우, 청크와 관련된 메타데이터에 특정 값이 없을 경우, 특정 날짜 이전의 청크일 경우 등 다양한 기준으로 청크를 필터링 할 수 있음

- **대안 #2 임베딩 벡터를 통해 청크 탐색후 KNN 수행**
    - 이미 검색된 청크와 잠재 공간에서 이웃 관계에 있는 다른 청크들을 추가하는 것

- **대안 #3 청크 재배치**
    - 청크를 단순히 관련성 순서대로 배치할 수 있지만, 교차 패턴으로 배치할 수도 있다.
    - 교차 패턴: 짝수 인덱스 청크는 리스트의 앞부분, 홀수 인덱스 청크는 뒷부분에 배치하는 방식
    - 넓은 범위의 상위 $k$개의 청크를 사용할 때 효과적이다.
    - 가장 관련성이 높은 청크를 입력 컨텍스트의 앞과 뒤에 배치하고 상대적으로 덜 중요한 청크는 중간에 위치하도록 만든다

#### 2.1.4.1. 컨텍스트 압축
**개념**: LLM의 응답 생성을 돕고 동시에 연산 자원을 절감하는 개념
**목표**: 문서 검색 후 컨텍스트를 압축하여 관련 정보만 보존

실제로 컨텍스트에는 <B>쿼리와 무관한 정보나 반복된 정보가 포함되어 있는 경우가 많다.</B> 또한 문장에서 대부분의 단어는 컨텍스트로부터 직접 예측할 수 있으므로 LLM이 응답을 생성하는 데 반드시 필요하지 않다.

|전략|내용|
|---|---|
|컨텍스트 필터링|\- 정보 이론에 따르면 엔트로피가 낮은 토큰은 예측하기 쉽고 중복 정보를 포함.<BR>\- 그래서 각 어휘 단위에 정보 값을 할당하는 LLM을 사용.<BR>\- 이를 바탕으로 내림차순으로 순위를 매기고, 사전에 결정하거나 컨텍스트에 따라 달리 설정한 $p$ 번째 백분위수 이내의 토큰만 남김.|
|LongLLMLingua|\- 엔트로피를 기반으로 하며 맥락과 쿼리 모두에서 정보를 사용하는 접근 방식.<br>\- 문서를 동적으로 압축하고 재배치함으로써 LLM의 으답 생성을 더 효율적으로 만든다.|
|오토컴프레서|\- 시스템의 파인튜닝과 요약 벡터를 활용<BR>\- 긴 텍스트를 작은 벡터표현으로 압축하고 이를 소프트 프롬프트로 사용해 모델에 컨텍스트를 제공.<BR>\- 이 과정에서 LLM의 가중치는 고정된 상태로 유지되며, 프롬프트에 삽입되는 학습 가능한 토큰만 훈련됨.<BR>\- 이 토큰들은 학습 과정에서 최적화되어 모델의 핵심 파라미터를 수정하지 않고도 시스템을 End-to-End 방식으로 최적화할 수 있다.<br>\- 생성 과정에서는 이러한 요약 벡터들이 결합되어 모델이 컨텍스트를 인식하게 된다.|


## 3. 모듈형 RAG와 다른 시스템 통합하기
<p align="center">
  <img src="https://github.com/user-attachments/assets/0861078a-989c-42d2-8137-078844940b21"><br>
  <b>Figure 1. RAG의 세 가지 패러다임 [1]</b> 
</p>

<b>개념</b>: 고급 RAG의 확장 개념으로, 적응성과 확장성에 초점을 맞춘 발전된 형태. <br>
<b>핵심</b>: `모듈형`이란 시스템을 개별 컴포넌트로 나누어, 이를 순차적 또는 병렬적으로 활용할 수 있음을 의미함.<br>
<b>목표</b>: 성능을 최적화하고 다양한 작업에 유연하게 대응할 수 있는 시스템을 만드는 것.

파이프라인 자체는 검색과 생성을 번갈아 하는 형태로 리모델링된다.

### 3.1. 모듈형 RAG의 구분

|주요 모듈|내용|
|--|----|
|검색 모듈|쿼리와 관련된 정보를 찾아내는 역할|
|메모리 모듈|검색 과정에서 관련 정보를 저장하는 역할|
|라우팅 모듈|쿼리에 가장 적합한 경로를 식별|
|생성 모듈|쿼리마다 요약, 의역, 맥락 확장 등 각기 다른 유형의 생성이 필요할 때 출력의 품질과 관련성을 높이는 데 기여|
|작업 적응 모듈|요청된 작업에 동적으로 적응하도록 시스템을 조정|
|검증 모듈|검색된 응답과 컨텍스트를 평가|

모듈형 RAG의 <b>장점</b>: 높은 적응성/유연성 → 필요에 따라 <u>모듈 교체 및 재구성 가능</u>, 모듈 간의 흐름을 세밀하게 조율 

|||
|-|-|
|나이브 RAG<BR>고급 RAG|검색 및 읽기 메커니즘|
|모듈형 RAG|검색, 읽기, 다시 쓰기 가능|

실제로 평가와 피드백 기능을 통해 시스템이 쿼리에 대한 응답을 정교하게 다듬을 수 있으며, 이 패러다임이 확산되면서 흥미로운 대안들도 등장했다.

#### 3.1.1. DSP (Demonstrate-Search-Predict)
<b>개념</b>: 복잡한 쿼리를 해결하기 위해 LLM과 RAG 간에 다양한 상호작용을 어떻게 구현할 수 있는지 보여준다.<BR>

또 DSP는 모듈형 RAG가 견고하면서도 유연한 파이프라인을 동시에 구현할 수 있음도 보여준다.

#### 3.1.2. Self-RAG
<b>개념</b>: 시스템에 비판적인 요소를 도입 <Br>
\- LLM은 <u>자신이 생성하는 내용을 반성</u>하고 <u>사실성과 전반적인 품질 측면에서 결과물을 비판</u>한다.<br>
\- 이러한 접근법들은 특히 <b>추론이 필요한 문제에서 강력한 효과를 발휘</b>한다. <br>

또 다른 대안 → CoT 생성과 검색을 교차 적용

### 3.2 훈련 기반 접근법과 비훈련 접근법
<b>비훈련 접근법 개념</b>: 시스템의 두핵심 구성 요소인 임베더와 LLM을 처음부터 고정된 상태로 유지하는 것 <br>
<b>훈련 기반 접근법</b>: 독립 훈련, 순차 훈련, 결합 훈련의 세 가지 유형으로 구분
|구분|내용|목적|
|---|---|---|
|독립 훈련|검색기와 LLM을 완전히 분리한 과정에서 각각 훈련. 즉, <b>시스템의 여러 구성 요소를 개별적으로 파인튜닝</b>.|특정 도메인(법률, 금융, 의료 등)에 시스템을 적합하게 조정하려는 경우 유용.|
|순차 훈련|<b>두 구성 요소를 순차적으로 활용</b>을 가정하는 훈련 방식을 취함.<br><br>\- <b>검색기 우선</b>: 검색기를 훈련시킨 후 우선 고정하고, 이후 LLM을 훈련시켜 검색기 컨텍스트 내의 지식을 어떻게 활용할지 익힌다.<br>\- LLM의 감독을 활용해 검색기를 훈련시킨다. 이는 <b>지식 증류의 한 형태로 볼 수 있다.|두 구성 요소 간의 시너지를 높임|
|결합 훈련|시스템을 End-to-End로 훈련하는</b> 방법. 즉, <b>검색기와 생성기를 동시에 정렬</b>하는 것|- 시스템이 지식을 찾는 능력과 해당 지식을 활용해 생성하는 능력을 동시에 향상<br>- 훈련 과정에서 두 능력 간의 시너지 효과를 얻을 수 있음|

일반적으로 나이브 RAG는 비훈련 접근법으로 간주함

<p align="center">
  <img src="https://github.com/user-attachments/assets/19105fad-e61a-4bde-afb4-f06c443e813d"><br>
  <b>Figure 2. RAG 훈련 방법 [2]</b> 
</p>

## 4. 고급 RAG 파이프라인 구현하기
<B>개념</B>: 고급 RAG 구현 → <U>나이브 RAG 방식을 확장하여 성능 향상을 위해</U> 여러 부가 요소를 결합한 개선 버전 <BR>
<b>핵심</b>: 임베딩과 검색, 생성으로 이루어진 <B>전통적인 RAG 파이프라인이 기본 구조</B>이며, 여기에 보다 <U>정교한 구성 요소들이 추가</U>됨.

|부가 요소|내용|
|--|----|
|**재순위화 모델**|검색 단계에서 찾아낸 컨텍스트를 정렬하는데 사용|
|**쿼리 변환**|단순한 쿼리 변환 기법을 적용|
|**쿼리 라우팅**|모든 쿼리를 동일하게 처리하지 않고, 보다 효율적인 검색을 위한 규칙 수립|
|**하이브리드 검색**|키워드 기반 검색과 시맨틱 검색을 결합하여 두 방식의 장점을 함께 활용|
|**요약**|검색된 컨텍스트에서 중복된 정보를 제거|

```python
def advanced_query_transformation(query):
    expanded_query = query

    movies_terms = ["movie", "film"]
    if not any(term in query.lower() for term in movie_terms):
        expanded_query = query + "movie"
    
    return expanded_query
```
- query 내에 영화 관련 일반 용어가 포함되어 있지 않을 시 추가.

- 쿼리 라우팅 단계 수행
```
if 질문에 특정 키워드 포함:
    키워드 기반 검색 수행
else:
    임베딩 기반 시멘틱 검색 수행
```
```python
def advanced_query_routing(query):
    textual_keywords = ["title", "named", "called", "specific"]

    if any(keyword in query.lower() for keyword in textual_keywords):
        return "textual"
    else: return "vector"
```
* 다음으로 하이브리드 검색 수행

- 시맨틱 검색과 키워드 검색을 모두 활용하며, 오늘날 RAG 파이프라인에서 가장 널리 사용하는 구성 요소 중 하나임.

- 문서를 청크 단위로 나눌 경우, 어떤 문서는 키워드를 통해서만 검색되는 경우도 있다.
    - 물론 <b><u>키워드가 포함되었다고 해서 반드시 관련 문서인 것은 아님</b></u>
    - 의미적 관련성이 더 중요한 쿼리에서는 더욱

- 하이브리드 검색을 사용하면 <u>두 가지 검색 방식을 균형 있게 조합</u>할 수 있으며, 각 검색 방식에서 얼마나 많은 청크를 가져올지를 선택할 수 있다.

```python
def fusion_retrieval(query, top_k=5):
    query_embedding = sentence_model.encode(query).tolist()
    vector_results = collection.query(
        query_embeddings=[query_embedding],
        n_results=min(top_k, len(documents))
    )

    tokenized_query = query.lower().split()
    bm25_scores = bm25.get_scores(tokenized_query)

    top_bm25_indices = np.argsort(bm25_scores)[-top_k:][::-1]
    bm25_documents = [documents[i]["content"] for i in top_bm25_indices]

    vector_docs = vector_results["documents"][0] if vector_results["documents"] else []
    combined_results = vector_docs + bm25_documents

    seen = set()
    unique_results = []
    for doc in combined_results:
        if doc not in seen:
            seen.add(doc)
            unique_results.append(doc)

    return unique_results[:top_k * 2]
```

- 재순위화 모델은 매우 자주 사용하는 요소로, 트랜스포머 모델을 활용하여 컨텍스트를 재정렬한다.

- 예를 들어, 10개 청크를 검색했다면 이들을 가장 관련성이 높은 순으로 재졍렬하고 그중 상위 일부만을 선택한다.
- 시맨틱 검색을 통해 가장 관련성 높은 청크를 찾더라도 이들이 낮은 순위에 위치한다면 모델 입력에 포함되지 않을 수 있다.
- 재순위화 모델은 이러한 청크들이 상위에 배치되어 LLM 입력 컨텍스트에 포함되도록 보장한다.

```python
def rerank_documents(query, documents_list):
    if not documents_list:
        return []
    
    pairs = [[quer, doc] for doc in documents_list]
    scores = rerank_model.compute_score(pairs, normalize=True)
    if not isinstance(scores, list):
        scores = [scores]
    
    ranked_docs = sorted(zip(documents_list, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked_docs]
```

- 컨텍스트에는 중복된 정보 포함될 수 있다.

- LLM은 노이즈에 민감하기 때문에 이를 줄이면 성능을 향상시킬 수 있다.

- 이를 위해 검색된 컨텍스트를 요약하며 정보를 간결하게 만드는 작업도 수행한다.

- 물론 이때도 지나치게 많은 정보를 잃지 않도록 요약 범위에 제한을 둔다.

```python
def select_and_compress_context(documents_list, max_docs=3):
    summariezd_context = []
    for doc in documents_list[:max_docs]:
        try:
            input_length = len(doc.split())

            if input_length < 30:
                summarized_context.append(doc)
                continue
            
            prompt = f"Summarize the following movie description concisely in 2-3 seqs: {doc} \n Summary:"
            response = chat_openai_model.invoke(prompt)
            summary = response.content
            summarized_context.append(summary)
        
        except Exception as e:
            summarized_context.append(doc)
    
    return summarized_context

```

- 이러한 구송 요소들이 모두 정의되면, 이들을 하나의 파이프라인으로 조립하기만 하면 된다.

- 조립이 완료되면 RAG 파이프라인을 실제로 활용할 수 있다.

- 작동하는 RAG 파이프라인을 확보했다면 그 다음 단계는 <B><U>배포</B></U>이다.

|파일|링크|비고|
|---|---|---|
|file.ipynb|[[**Link**]()]||
|file.ipynb|[[**Link**]()]||

## 5. RAG의 확장성과 성능 이해하기
이번 절에서는 RAG 시스템을 실제로 운영에 도입하고 확장하는 과정에서 마주칠 수 있는 과제를 살펴본다. 
- RAG의 가장 큰 장점은 LLM과 달리 별도의 추가 훈련 없이도 확장할 수 있다는 점이다. 
- <b><u>개발과 프로덕션의 목적과 요구 사항은 크게 다르다.</b></u>
- RAG나 LLM 같은 시스템은 실제 프로덕선 환경으로 이전하는 과정 (프로덕션화 단계)에서 새로운 도전에 직면한다.
- 프로덕션화란 RAG와 같은 복잡한 시스템을 단순한 프로토타입 단계에서 안정적으로 운영 가능한 환경으로 이관하는 것을 뜻한다.
- 원격으로 접속하는 다양한 사용자를 관리해야 하는 상황이라면 이 과정은 더욱 복잡해질 수 있다.
- <b>개발 단계에서는 정확도가 가장 중요한 지표일 수 있지만, <u>프로덕션 환경에서는 성능과 비용의 균형을 세심하게 고려</u>해야 한다.</b>
- 대규모 조직의 경우 이미 방대한 데이터를 보유하고 있을 수 있으며, 이러한 빅데이터와 함께 RAG를 활용하고자 할 가능성이 높다.
- 그러나 데이터의 양, 속도, 다양성을 고려할 때, 빅데이터는 RAG 시스템에 있어 상당한 도전 과제가 된다.
- 결국 <b><u>확장성은 빅데이터뿐 아니라 RAG 시스템 전반에서 중요한 고려사항이다.</b></u>

### 5.1. 데이터 확장성, 저장, 전처리
지금까지는 주로 정보를 어떻게 검색할 것인지에 대해 공부하였고, 데이터가 텍스트 형태인 것을 가정하였다. 실제로는 <u>텍스트 데이터 구조가 중요한 변수</u>이며 <b>이를 프로덕션 환경에 적용할 때 여러 문제가 발생할 수 있다.</b> 따라서 시스템은 다음과 같은 유형의 데이터를 통합할 수 있어야한다.

|유형|정의|특징|예시|
|---|---|---|---|
|비정형 데이터 (Unstructured data)|코퍼스에서 가장 일반적인 데이터 유형. 생성 주체가 사람일 수도 있고 자동화 시스템 및 LLM일 수 있다.|다국어로 구성된 경우 교차 언어 검색 수행 필요|위키백과와 같은 백과사전형 데이터, 과학$\cdot$의료$\cdot$금융 등 도메인 특화 데이터, 산업 보고서, 인터넷에서 수집된 텍스트, 사용자 채팅 등|
|반정형 데이터 (Semi-structured data)|일반적으로 텍스트와 표 형식의 정보가 섞여 있는 데이터|RAG에서 다루기 복잡한 경우가 많음. 여전히 활발히 연구가 진행되는 중|PDF, JSON, XML, HTML 등|
|정형 데이터 (Structured data)|사람과 소프트웨어가 효율적으로 접근할 수 있도록 표준화된 형식으로 저장된 데이터|(1) 정의된 속성 [예: 테이블]<br>(2) 관계형 속성 [예: 테이블 간의 공통 값을 통해 서로 다른 데이터셋을 연결]<br>(3) 정량 데이터 [예: 수학적 분석에 최적화]<br>(4) 저장 형식 [예: 특정 포맷과 명확한 규칙에 따라 저장]|Excel, SQL 데이터베이스, 웹 양식 결과, POS 데이터, 상품 디렉터리 등|

#### 예시
- 여러 지역과 다양한 언어에서 규정 준수 문서를 검색해야 하는 시스템을 설계한다면, 교차 언어 검색이 가능한 RAG가 필요하다.

- 조직 내 데이터가 대부분 PDF나 SQL 같은 특정 형식이라면, 해당 형식에 최적화된 검색 파이프라인을 설계해야 한다.
    - 구조화된 데이터로 RAG의 성능을 향상 시키기 위한 구체적인 대안
        - 체인 오브 테이블 (Chain-of-table): CoT 프롬프팅과 테이블 변환을 통합하는 방식
            - LLM과 사전에 정의된 연산집합을 사용하여 복잡한 테이블을 단계적으로 추출하고 연산을 적용
            - 복잡한 SQL 데이터베이스나 대규모 데이터프레임을 데이터 소스로 사용할 때 특히 유용

RAG 성능에 영향을 주는 것은 <U>데이터의 유형뿐 아니라 <B>데이터의 양</B> 그 자체</U>도 있다. 데이터의 양이 증가할수록 관련 정보를 찾는 것도 어려워지며 시스템의 지연 시간도 증가하는 경향이 있다.

#### 5.1.1. 데이터 저장

- **데이터 저장**: 시스템을 프로덕션 환경으로 이관하기 전에 반드시 다루어야 할 핵심 요소 중 하나
- 분산 저장 시스템: 데이터를 여러 물리적 서버나 데이터 센터에 분산해 저장하는 인프라
    - <u>대규모 데이터를 처리하는 하나의 해법</u>이 될 수 있다.
    - 시스템 속도를 향상시키고 데이터 손실 위험을 줄이는 장점이 있지만, 동시에 <b>비용 증가 및 관리 복잡성이라는 리스크를 동반</b>한다.
- 데이터 유형이 다양할 경우에는 <b>데이터 레이크</b>라는 구조를 활용하는 것이 유리하다.
    - 데이터 레이크는 정형, 반정형, 비정형 데이터를 저장하고 처리하도록 설계된 중앙 저장소이다.
    - <b>장점</b>
        - 다양한 유형의 데이터를 수집, 처리, 저장할 수 있는 확장 가능하고 유연한 구조
        - RAG 환경에서는 <B>다른 데이터 구조보다 <U>더 많은 데이터 컨텍스트를 유지</U>할 수 있음</B>
    - <B>단점</B>: 제대로 운영하려면 고도의 전문성이 필요함

- (또 다른 대안) 데이터를 지리적 위치, 주제, 시간 등 기준에 따라 더 작고 관리하기 쉬운 파티션으로 분할하여 효율적인 검색을 가능하게 하는 방법
- 요청이 많은 경우에는 자주 조회되는 데이터를 캐싱하여 반복 검색을 피할 수 있다.

#### 5.1.2. 데이터 전처리
<B>핵심</B>: 데이터 저장과 더불어 파이프라인을 탄탄하게 구축하는 것이 매우 중요한 단계
- 개발 단계에서는 정제된 고품질 데이터셋을 사용하는 경우가 많다.
- <U>프로덕션 환경</U>, 특히 빅데이터 환경에서는 <B>데이터 불일치를 제거하거나 결측치와 불완전한 데이터를 처리해야하는 경우</B>가 많다.
- 결측치 보간 (내삽: Interpolation / 외삽: Extrapolation), 이상치 탐지, 정규화, 정규 표현식 기반 오류 제거 등의 기법을 추가로 적용해 노이즈와 오류 데이터를 제거할 수 있다.

#### 5.1.3. 데이터 중복
<b>핵심</b>: LLM을 다룰 때 반드시 고려해야 하는 항목.
- 중복 데이터는 LLM의 학습 효율을 떨어뜨릴 뿐 아니라, 생성 과정에서도 부정확하고 편향되며 품질이 낮은 출력을 유발할 수 있다.
- 데이터 양이 많아질수록 중복 발생 위험도 선형적으로 증가한다.

- 방지책: 퍼지 매칭, 해시 기반 중복 제거 기법 활용
    - 일반적으로, 시스템 내 데이터 품질과 거버넌스를 관리하기 위한 품질 관리 파이프라인을 구축하는 것이 바람직함.
    - 해당 파이프라인에는 문제 데이터를 식별하고 그 출처를 추적할 수 있는 규칙과 추적 시스템이 포함되어야 한다.
        - 단 , 파이프라인이 지나치게 복잡하거나 시스템 속도를 저하시키는 구조는 피해야 한다.

----
데이터 저장 인프라가 정해졌다면, 이후에는 효율적인 데이터 인덱싱 및 검색 체계를 구축해야 한다. 빅데이터에 특화된 인덱싱 방법으로는 아파치 루씬이나 엘라스틱서치와 같은 기술이 있다. 또한 자주 검색되는 데이터는 캐싱하고 검색 과정을 분산하여 병렬 인프라를 구성하면, 사용자 수가 많을 때 병목 현상을 줄일 수 있다. 이러한 기술적 요소들은 복잡하기 때문에, 프로덕션 환경에 적용하기 전에 반드시 테스트와 벤치마킹을 수행하는 것이 바람직하다.

### 5.2. 병렬 처리
<p align="center">
  <img src="https://github.com/user-attachments/assets/9fdb98dd-b390-457a-9944-1eac99817c2b"><br>
  <b>Figure 3. CPU와 GPU 구조 [3]</b> 
</p>

- RAG에 병렬 처리를 적용하면 대규모 데이터셋을 다루는 상황에서도 시스템의 지연 시간을 크게 줄일 수 있다.

- 병렬 컴퓨팅은 RAG 파이프라인의 여러 단계 (저장, 검색, 생성)에 적용할 수 있다.
    - **저장 단계**: 여러 노드를 활용해 데이터 전처리 파이프라인 전체를 병렬로 수행 가능
        - 전처리 파이프라인에는 데이터셋 일부의 전처리와 인덱싱, 청크 분할, 임베딩까지 포함
    - **검색 단계**: 데이터셋을 여러 노드로 분할하고 각 노드가 특정 데이터셋 샤드에서 정보를 찾도록 할 수 있다.
    - **생성 단계**

LLM은 연산 집약적이지만 트랜스포머 기반이므로, 원래부터 훈련과 추론의 병렬 처리를 모두 고려하여 설계되었다. 긴 시퀀스나 대규모 배치의 경우에도 병렬 처리가 가능한 기술들이 존재하며 이후에는 <U>텐서 병렬화, 모델 병렬화, 특화 프레임워크 등 더욱 정교한 기법들도 개발</U>되었다.

- 시스템 병렬화에는 본질적인 어려움과 새로운 오류가 발생할 위험 존재
    - 시스템 사용 중에는 <U>지속적으로 모니터링</U>하고 체크포인트와 같은 내결함성 메커니즘, 동적 작업 할당과 같은 고급 스케줄링 기법 그리고 기타 가능한 해결책들을 구현하는 것이 중요하다.

- RAG는 자연 집약적인 프로세스이므로, 동적으로 자원을 할당하고 프로세스별 워크로드를 모니터링하는 기술을 함께 적용하는 것이 바람직하다.
- 데이터 수집과 저장, 검색, 생성을 모듈화하여 구성 요소 간의 독립성을 확보하는 것이 좋다.
- 어떠한 경우에도 정확도와 같은 성능뿐만 아니라 메모리 사용량, 비용, 네트워크 사용량 등을 함께 모니터링하는 프로세스를 갖추는 것이 좋다.
    - <B><U>개발 환경과 프로덕션 환경 모두 고려</B></U>

- 병렬 효율에 대한 고민

#### 시스템에는 많은 대안이 존재하므로, 어떤 요소가 가장 중요하고 필요한지 결정하는 것은 쉽지 않다.

<p align="center">
  <img src="https://github.com/user-attachments/assets/91451812-9f7e-4991-a59e-16c52a949f57"><br>
  <b>Figure 4. 개별 모듈 및 기술이 정확도와 시간 지연에 미치는 영향 [4]</b> 
</p>

모든 구성 요소를 체계적으로 테스트하기는 어렵지만 다음과 같은 지침은 유용할 수 있다. 최적의 성능은 쿼리 분류 모듈, HyDE, 재순위화 모듈, 컨텍스트 재구성 그리고 요약을 조합해서 사용할 때 달성된다.

그러나 연산 비용이나 지연 시간이 부담스러우면 HyDE는 생략하고 나머지 모듈만 사용하는 것이 더 낫다.

### 5.2. 보안과 개인정보 보호
시스템을 프로덕션 환경으로 이관할 때 반드시 고려해야 할 핵심 요소는 <B>보안</B>과 <B>개인 정보 보호</B>이다.

RAG 시스템은 민감하고 기밀성이 높은 데이터를 대규모로 다룰 수 있기 때문에 시스템이 침해될 경우 규제 벌금, 소송, 평판 손상 등 조직에 막대한 피해를 초래할 수 있다.

이러한 위험을 방지하기 위한 대표적인 방법 중 하나는 <B>데이터 암호화</B>이다. 업계에서 널리 사용되는 알고리즘과 프로토콜은 RAG 시스템에도 동일하게 적용할 수 있다. 또한 암호화 키 관리 정책을 마련하고 키를 주기적으로 변경하는 등의 내부 보안 정책도 중요하다.



## Reference
[1] Gao, Yunfan, et al. "Retrieval-augmented generation for large language models: A survey." arXiv preprint arXiv:2312.10997 2.1 (2023).<br>
[2] Fan, Wenqi, et al. "A survey on rag meeting llms: Towards retrieval-augmented large language models." Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining. 2024.<br>
[3] Y. Han, 챕터1_GPGPU_및_병렬처리, Kaggle. [Online] https://www.kaggle.com/code/yoseobhan/1-gpgpu (Accessed Feb 13th 2026)<br>
[4] Wang, Xiaohua, et al. "Searching for best practices in retrieval-augmented generation." Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.<br>