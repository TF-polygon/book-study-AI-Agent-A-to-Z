# 3장: 강력한 AI 엔진, LLM 탐구하기

핵심 주제: 초거대 트랜스포머를 방대한 데이터셋으로 학습시키기 위한 다양한 학습 기법 및 멀티모달 데이터를 위한 데이터 확장 기법 알아보기

- LLM의 진화 과정 살펴보기
- 지시 튜닝, 파인튜닝, 정렬
- 작고 효율적인 LLM 탐구하기
- 멀티모달 모델 탐구하기
- 할루시네이션과 윤리적 $\cdot$ 법정 쟁점 이해하기
- 프롬프트 엔지니어링

## 1. LLM의 진화 과정 살펴보기
<b>LLM (Large Language Model)의 정의</b>: 100억 개 이상의 파라미터를 가진 모델

인간의 언어를 이해하고 생성하는 것에 그치지 않고 코드 생성 등 더 다양한 능력을 갖추기 위해, 단순히 파라미터 수만 늘리는 것이 아니라 <u><b>방대한 양의 데이터로 학습</b>해야 한다.</u>

주어진 이전 문맥으로 다음 단어를 예측하는 문제 <b>자기회귀 언어 모델링</b>를 통해 학습된다.

#### 파라미터 수 증가 배경
- 학습 가능성: 스케일링 법칙에 따라, 파라미터 수가 많을수록 더 뛰어나 능력을 갖출 수 있음
- 표현력: 모델 내부에서 더 복잡한 수식을 표현할 수 있어 일반화 능력이 향상됨. 과적합 위험이 줄어듦.
- 기억력: 파라미터가 많아질수록 더 많은 지식 내재화

### 1.1. 스케일링 법칙
<b>핵심</b>: 모델 크기(파라미터 수), 데이터셋 크기, 컴퓨팅 자원(훈련 비용)이 커질수록 대형 언어 모델(LLM)의 성능이 예측 가능하게 향상된다는 원리 (손실 관점)
```
예시 문장: The sentiment of the sentence: 'I Like Pizza' is
예측 단어: 'positive' or 'negative'
```
#### 조건부 확률
$$
P(\rm{positive~~|~~The~sentiment~of~the~sentence:~'I~like~Pizza'~is})
$$
$$
P(\rm{negative~~|~~The~sentiment~of~the~sentence:~'I~like~Pizza'~is})
$$

|예시|설명|
|---|---|
|질의응답|주어진 질문에 대해 정답 (답변)으로 가장 적절한 토큰의 확률을 계산하는 문제|
|텍스트 요약|원문을 주고 요약문의 확률을 생성하는 문제|

```
질의응답: P(answer | question)
텍스트 요약: P(summary | original article)
```
언어 모델링을 활용하면 거의 모든 과제를 해결할 수 있다.

이러한 능력을 갖춘 LLM 학습을 위해, 특별히 구성된 초대형 데이터셋이 사용된다.
- GPT3 학습 데이터셋: Common Crawl(웹 크롤링 데이터, 4,100억 토큰), Books2 & Books2(도서 코퍼스, 각각 120억/550억 토큰), Wikipedia(30억 토큰)

#### 파라미터 수 결정 요인
|결정 요인|설명|
|----|-----|
|임베딩 레이어|트랜스포머 기반 언어 모델이 구사할 수 있는 모든 단어 각각을 벡터로 변환하여 만들어진 행렬. 이 벡터들은 전부 학습 가능하므로 모델의 전체 파라미터 수에 포함됨|
|Self Attention|여러 가중치 행렬을 포함, 컨텍스트 길이가 길어질수록 크기도 증가|
|깊이|트랜스포머 블록 수를 늘리면 파라미터 수도 직접적으로 증가|

#### 1.1.1. LLM 성능 결정 요인
- 모델 크기 (파라미터 수)
- 데이터 크기 (학습 데이터셋 규모)
- 연산 크기 (컴퓨팅 자원)

성능 향상 조건: 모델의 크기 증가, 학습 데이터셋 증가, 더 많은 에포크로 학습 수행

### OpenAI는 이 요인들을 스케일링 법칙으로 정리

파라미터 수 $N$, 데이터셋 크기 $D$, 연산량 $C$에 대한 손실 함수:

$$
\mathcal{L}(N)=\big( \frac{N_C}{N} \big)^{a_N}~~~~~~\mathcal{L}(D)=\big( \frac{D_C}{D} \big)^{a_D}~~~~~~\mathcal{L}(C)=\big( \frac{C_C}{C} \big)^{a_C}
$$

<p align="center">
  <img src="https://github.com/user-attachments/assets/110f1308-5f0d-4852-8ba8-29e397535783"><br>
  <b>Figure 1.</b> 연산량, 데이터셋, 모델 크기 증가에 따른 언어 모델링 성능 향상 [1]
</p>

- 스케일링 법칙을 통해 <b>비가역적 손실</b>과 <b>가역적 손실</b>로 분해하여, 학습 전부터 모델의 예상 성능을 추정할 수 있다.
- 손실을 줄이고 성능을 개선하기 위해 모델을 확장할 것인지, 데이터셋을 늘릴 것인지 사전에 결정할 수 있다.
- 단, 이 상수들은 모델의 아키텍처와 기타 학습 설정에 따라 달라질 수 있다.

#### 스케일링 법칙에 대한 지적사항
- OpenAI의 주장보다 토큰 수에 훨씬 더 민감하다는 지적
- 더 학습할 수 있는 여지가 있음에도, <u>적은 학습 데이터를 적은 토큰으로 학습하여 과소적합 상태일 가능성</u>
- 단순히 많은 양의 토큰이 아닌 양질의 토큰이 필요
- <u>모든 토큰이 동일하게 가치있는 것은 아님</u>
- 다른 모델이 생성한 토큰을 사용하는 것은 고도화된 형태의 지식 증류에 불과하다는 지적

#### 1.1.2. 합성 데이터
<b>정의</b>: 실제 데이터를 모방한, 인간이 생성하지 않은 데이터 (생성형 인공지능 기술을 기반으로 한 컴퓨팅 알고리즘 및 시뮬레이션을 통해 생성)

의료 데이터는 특히 법률에서 민감 정보로 규정하고 있는 만큼, 많은 학습 데이터를 요구하는 의료 인공지능 분야에서 데이터 수집에 큰 어려움이 있음
- 가명화 프로세스의 등장에 의료 데이터 수집에 돌파구로 작용할 것으로 기대를 한 몸에 받았으나, <u>각종 사용 규제 및 가명화된 의료 데이터 재식별 위험 등의 문제</u>로 여전히 어려움이 있음

### 프라이버시와 규제로부터 비교적 자유로울 수 있는 의료 합성 데이터가 주목

<br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/649dc591-d509-445b-9f9b-45cae6609d7e"><br>
  <b>Figure 2.</b> 실제 흉부 X-ray와 합성 흉부 X-ray [2]
</p>

합성 데이터는 원데이터로 재식별되지 않도록 가능성을 최소화하여 가명화 이전 정보에 대한 <b>역추적 위험이 훨씬 적기 때문에</b> 정보 보호 문제로부터 비교적 자유롭다. 

<b>희귀하여 수집이 어려운 데이터</b> 등의 임상 데이터를 <b>합성 데이터를 통해 충분항 양으로 수집 가능하며 이를 통해 <u>부족한 다양성의 문제를 해결할 수 있다</b>는 데에 긍정적</u>이다.

### GAN (Generative Aversarial Network) (Reference)
<hr>
<code style="color : red"> None </code>
<hr>

데이터 확보의 한계를 절대적으로 해결할 수 있을 것 같은 이 '<b>합성 데이터</b>'에도 문제가 존재함.
- 모델 붕괴 - 모델 성능이 급격하게 저하되는 현상 <br>
→ <B>사람의 창작물이 아닌</B> AI가 생성한 결과물로 채워지고, AI는 이를 통해 스스로 학습함으로써 <B>창의성 상실 및 품질 저하 발생</B>
- 파괴적(파멸적) 망각 - 이미 학습했던 일부 기술을 잊어버리는 현상

그럼에도 스케일링 법칙은 <u>여전히 중요한 의미를 지닌다.</u>

1,000억 개 이상의 파라미터를 갖는 대형 모델을 위한 설계, 시간, 비용 측면에서 방대한 투자가 필요하므로, 소형 모델로 실험하는 것이 합리적

최근, 특정 작업에 최적화된 맞춤형 스케일링 법칙 연구가 활발하다.

ex) [Code assistant model] → 전체 성능보다 코드 관련 과제에 대한 성능이 더 중요

### 1.1.3. 창발적 특성
<b>개념</b>: 작은 모델에서는 안나타나는 현상이 일정 규모에 도달하면 갑작스럽게 발현되는 특성<br>
\- 모델은 일정 크기에 도달하기 전까지는 창발적 능력이 무작위 수준에 불과하다가 <b>임계 규모</b>에 도달하는 순간 본격적으로 발현 <br>
\- <b>상전이 현상</b>: 특정 임계정을 지나면서 거의 0에 가까웠던 성능이 최고 수준으로 급격히 향상 <br>
\- 보통 수학적 추론이나 다단계 추론처럼 복잡한 과제에서 발생. <br>
\- <b>충분히 큰 모델을 만들어야 하는 필요성을 뒷받침. </b><br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/ecdf2d7d-e5d3-4cec-8dbe-dadbff67628b"><br>
  <b>Figure 3.</b> 다양한 LLM 계열에서 발생하는 창발적 특성 현상 [3]
</p>

### 1.1.4. 컨텍스트 길이
LLM은 일정 크기의 청크, 고정된 크기의 컨텍스트 윈도우 단위로 텍스트를 입력 받아 처리한다. <br>
<b>컨텍스트 길이</b>: 모델이 한 번에 처리할 수 있는 정보의 양을 결정. 길어질수록 텍스트 내 장거리 의존성을 포착하는데 유리하며, 이는 여러 특정 작업에서 성능 향상으로 이어짐.

- <b>문서 요약</b>: 더 많은 컨텍스트를 반영해 일관되고 간결한 요약을 생성하며, 문서 전체에 걸쳐 엔티티와 그 관계를 파악.
- <b>질의응답</b>: 정답에 이르는 복잡한 관계를 추적할 수 있으며, <u>멀티-턴 대화에서도 이전 질문과 답변을 기억</u>.
- <b>언어 번역</b>: 특히 긴 문서나 복잡한 뉘앙스가 있을 때 맥락을 더 잘 유지함. 컨텍스트 길이가 길면 전문 문서, 기술 용어, 다의어, 약어 등을 번역하는 데 도움이 됨
- <b>대화형 AI</b>: <u>대화 전체 흐름을 더 정확히 추적하며 자연스러운 상호작용을 유지</u>함.

### 1.1.5. 전문가 혼합
개념 (MoE; Mixture of Experts): <u><b>연산 비용이 제한된 상황</u></b>에서 더 큰 모델을 적은 학습 단계로 훈련하기 위해, <u>동일한 연산 비용으로 모델을 확장</u>할 수 있게 해주는 기법

|구성 요소|설명|
|-|-|
|희소 MoE 레이어|각 레이어는 여러 전문가로 구성되며, 각 전문가는 하나의 신경망이다. 가장 단순한 형태는 Feedforward Network이다.|
|게이트 네트워크/라우터|입력 데이터를 어떤 전문가에게 보낼지를 결정한다. LLM의 경우 라우터는 특정 토큰을 하나 이상의 전문가에게 분배한다. 라우터 자체도 학습 가능한 파라미터를 가지며, 사전 학습 과정에서모델의 다른 부분과 함께 훈련된다.|

<p align="center">
  <img src="https://github.com/user-attachments/assets/43251afc-ee42-41af-9e68-fa59533cf7fc"><br>
  <b>Figure 4.</b> MoE 레이어의 예시 [4]
</p>

<b>MoE 희소 연산의 장점</b>
- 사전 학습 속도가 밀집 모델 (전통적인 트랜스포머)보다 빠르며, <u>추론 단계에서 모든 전문가를 동시에 사용할 필요가 없어 속도가 빠름</u>.
- 시스템이 유연하여 복잡한 분포를 처리할 수 있고, <u>각 전문가는 특정 하위 도메인에 특화</u>될 수 있음.
- 필요에 따라 전문가를 더 추가할 수 있어 확장성이 뛰어남.
- (<b>집단 지성</b>) <u>여러 전문가의 예측을 평균 내어 더 나은 일반화 성능을 얻을 수 있음</u>.

<b>MoE 희소 연산의 단점</b>
- <u>모든 전문가를 메모리에 적재해야 하므로 VRAM 사용량이 큼.
- 학습이 더 복잡하고 과적합을 일으킬 수 있음.
- 별도의 조치가 없다면 모델이 두세 개의 인기 있는 전문가만 사용할 위험이 큼.
- 파인튜닝이 어려움.
- 구성 요소가 늘어남에 따라 모델의 해석 가능성이 더 복잡해짐.</u>

### 최근, GPT-4, Gemini와 같은 대규모 모델 상당 수가 MoE 구조를 채택하고 있음.

## 2. 지시 튜닝, 파인튜닝, 정렬
<b>전통적인 파인튜닝</b>: 모델의 가중치를 특정 작업이나 새로운 도메인에 맞게 조정하는 과정 <br>
<u>1,000억 개가 넘는 파라미터를 가진 대규모 모델의 모든 파라미터를 학습하는 것보다 1억개만 학습하고도 유사한 효과를 내는 것</u>이 인프라와 비용 차원에서 효율적이다.

모델 대부분의 가중치를 동결한 상태에서 일부 가중치만 파인튜닝을 수행할 방법이 필요하다.

### 2.1. 내재 랭크 가설
<b>핵심</b>: 신경망에서 일어나는 중요한 변화를 저차원 표현으로도 충분히 포착할 수 있다는 것
$$
Y=W'X~~이때~~W'=W+\Delta W
$$
$\Delta W$는 파인튜닝 과정에서 업데이트 되는 가중치 변화. <BR>
내재 랭크 가설에 따르면, <u>$\Delta W$의 모든 요소가 중요한 것은 아니므로</u>, $\Delta W$를 더 작은 차원의 두 행렬 $A$와 $B$의 곱으로 표현할 수 있다. 이로 인해, 기존 모델의 가중치는 그대로 동결한 채, <b><u>이 두 행렬만 학습하면 된다</u></b>.
$$
Y=W'X~~여기서~~W'=W+BA
$$

#### 2.1.1. 행렬 분해
<code style="color : red"> None </code>

#### 2.1.2. LoRA: 딥러닝에서의 행렬 분해
<code style="color : red"> None </code>

#### 2.1.3. LoRA 학습의 주요 장점
- 학습 효율이 뛰어나다.
- 추론 단계에서 연산 비용이 증가하지 않는다.
- 기존 모델의 원래 능력을 훼손하지 않는다.
- 다양한 애플리케이션과 도메인에 맞춘 변화 행렬을 별도로 만들 수 있다.

### 2.2. 어댑터 추가 방식
<b>핵심</b>: 트랜스포머 블록 안에 파인튜닝 가능한 작은 레이어를 추가하여, 이 추가된 파라미터만 학습하는 기법 <b>(오토인코더 구조와 유사)</b>

### Why? : 어댑터 구조 vs 오토인코더 구조
#### 구조적 유사성
|구분|오토인코더|어댑터|유사 의미|
|---|------|------|------|
|입력부|고차원 입력 데이터 $(x)$|기존 모델의 중간 출력값 $(h)$|처리할 원본 정보의 유입|
|압축|데이터를 저차원의 잠재적 공간으로 압축|고차원 벡터를 작은 차원으로 축소|차원의 저주 해결 및 핵심 정보 요약|
|중간층|<b>Bottleneck</b>: 가장 핵심적인 특징만 남은 상태|<b>Bottleneck</b>: 파라미터를 최소화하여 학습 효율을 극대화한 구간|최소한의 자원으로 정보의 정수 추출|
|비선형성|ReLU, Sigmoid 등|ReLU, GeLU 등|복잡한 패턴 학습 가능성 부여|
|복원|압축된 정보를 다시 원래 데이터 크기로 복원|축소된 벡터를 기존 모델의 차원으로 복원|주변 시스템 (Layer)과의 규격 호환|
|출력부|재구성된 데이터 ($\hat{x}$)|변형된 출력값 ($h'$)|기존 흐름에 맞게 가공된 결과물 전달|

#### 동작 원리의 공통점
|구분|오토인코더|어댑터|
|-|-|-|
|목적|데이터의 중요한 특징(Latent) 추출 및 복원|새로운 작업(Task)에 필요한 핵심 지식만 학습|
|압축|입력 데이터의 차원을 축소하여 핵심만 남김|모델 레이어 사이에서 특징 맵의 차원을 축소|
|복원|축소된 데이터로 원본 데이터를 재구성|원래의 임베딩 차원으로 맞추어 다음 레이어로 전달|
|수학적 형태|$Output=g(f(Input))$|$h'=h+W_{up}(f(W_{down}(h)))$|
<BR>

<B>어댑터의 예시</B> 
- FC layer의 차원이 1,024라면 어댑터는 이를 24차원으로 축소
- 다시 1,024차원으로 복원하도록 곱할 수 있는 가중치 행렬을 추가
- 이 경우 어댑터 하나당 5만 개 미만의 파라미터만 추가되는 셈

\- 어댑터 추가 시 <b>BERT 모델 전체를 파인튜닝하는 것과 동일한 성능을 얻을 수 있음</b>을 보임<BR>
<U>훨씬 더 적은 계산 자원으로 모델의 모든 파라미터를 학습하는 것과 같은 성능을 달성</U>할 수 있음.<BR>
LLM 차원에서도 <U>수백만 개 정도의 적은 파라미터만 학습해도 파인튜닝이 가능</U>하며, 동시에 <B>모델의 원래 능력을 그대로 유지할 수 있다.</B>

### 2.3. 파라미터 훈련 없이 파인튜닝하는 기법
<b>프롬프트 튜닝</b><br>
<B>핵심</B>: 입력 임베딩 앞에 학습 가능한 텐서를 추가하여 새로운 작업의 특성을 학습

<b>프리픽스 튜닝</b><br>
<B>핵심</B>: 모든 레이어의 은닉 상태에 학습 가능한 텐서를 추가

\- 두 방식 모두 나머지 파라미터는 고정한 채 경사하강법으로 학습 <br>
\- 다만 학습 중 불안정성이 발생할 수 있어 실제로는 LoRA 학습 및 어댑터 추가 방식이 가장 널리 활용됨

### 2.4. 인간의 피드백을 활용한 LLM 최적화
#### 2.4.1. 정렬 (Alignment)<br>
<b>개념</b>: 추가 학습을 통해 LLM을 인간의 가치에 맞추려는 방법
<b>핵심</b>: 수학적 학습 목표와 인간이 기대하는 <u><b>도움이 되고 정직하며 무해한 특성</b> 간의 간극을 줄이는 것</u>

<p align="center">
  <img src="https://github.com/user-attachments/assets/823e4937-3233-470a-8dfb-96fca9ed8194"><br>
  <b>Figure 5.</b> 정렬 전후 출력 비교 [5]
</p>

고도화된 자동 완성 LLM을 위해 방대한 지식과 다양한 능력을 학습하는데, <u>이러한 능력을 <b>인간의 가치와 조화를 이루어</b> 활용</u>할 수 있게 <b><u>정렬</b></u> 과정을 도입.<br>
- 인간의 가치는 주관적
- 수학적으로 표현하기 어려움<br>

∴ 인간의 피드백을 활용하는 방법 고안  

#### 2.4.1.1 인간 피드백 기반 강화학습
- <B>RLHF</B> (Reinforcement Learning from Human Feedback): 강화학습을 통해 LLM을 인간의 피드백에 맞게 최적화

|단계|설명|
|-|-|
|지도 파인튜닝 (SFT)|프롬프트 목록 우선 선정 후 <u>인간 주석자가 각 프롬프트에 대응하는 출력(응답)을 작성</u>. 이를 기반으로 사전 학습된 LLM을 파인튜닝하여 사람의 응답을 모방하는 SFT로 학습된 LLM을 만든다.|
|보상 모델 학습|일련의 프롬프트 선택 후, SFT LLM을 사용하여 각 프롬프트에 대해 여러 개의 출력을 생성. 이후 <u>인간 주석자가 이를 선호도 기준으로 순위화</u>. 이 순위를 바탕으로 보상 모델을 학습. 보상 모델은 LLM 출력을 입력받아 <u>인간 선호도와의 일치도를 스칼라 수치로 반환</u>한다.|
|강화학습 (RLHF)|프롬프트를 넣고 SFT LLM으로 출력을 생성한 뒤, 보상 모델로 보상을 예측. 이후 <b>강화학습 알고리즘 (PPO)을 활용해 SFT LLM을 보상에 맞추어 업데이트</b>. 이때 쿨백-라이블러 발산에 기반한 패널티 항을 추가하여, 모델이 원래 분포에서 지나치게 벗어나지 않도록 한다.|

#### Limitations of RLHF
- (<b><u>주석자 고용 및 품질 관리 필수</b></u>) 인간 선호 데이터를 수집하는데 비용이 많이 듦
- RLHF 과정 자체가 복잡하고 불안정적.

#### 2.4.1.2. 직접 선호 최적화
- <b>DPO</b> (Direct Preference Optimization)<br>
<b>핵심</b>: 보상 모델을 따로 학습하지 않음으로써 RLHF의 일부 문제를 해결

<p align="center">
  <img src="https://github.com/user-attachments/assets/c9d756bb-5c88-4ea6-b9ea-06cedde0a85f"><br>
  <b>Figure 6.</b> 강화학습 없이 최적화하는 DPO [6]
</p>

\- 데이터셋을 [프롬프트, 나쁜 응답, 더 나은 응답] 형식으로 구성 <br>
\- 더 나은 응답의 확률을 높이고 나쁜 응답의 확률을 낮추도록 손실함수 설계 <br>
\- 역전파만으로 학습할 수 있어 강화학습을 거치지 않아도 된다.

### 2.3. 지시 튜닝
<b>개념</b>: 다양한 작업에서 모델의 능력을 향상시키고, 특히 지시를 따르는 능력을 강화하기 위해 사용하는 파인튜닝 기법 <br>
<b>핵심</b>: <u>사용자의 지시를 직접 실행하는 것과 대규모 텍스트 코퍼스에서 다음 단어 예측만을 학습하는 것 간의 <b>불일치 문제를 해결</b>하기 위해 제안</u> <br>
<b>특징</b>: 기본 원리는 정렬과 유사. <b>'지시-출력 쌍'으로 구성된 데이터셋으로 추가 학습</b>하는 방식<br>
\- (기본 아이디어) 모델이 주어진 작업을 기대된 출력과 함께 학습 <br>
\- 모델은 기대 출력과 비교해 평가되고 이를 통해 최적화된다.

<b>활용과제</b>: (61가지 이상) 질의응답, 요약, 분류, 번역, 창작 글쓰기 등이 대표적 <br>
\- <u>데이터셋 내 작업의 <b>다양성이 클수록</b> 효과적</u> <br>
\- 추론 능력이 요구되거나 단계별 지시가 포함된 작업에서 성과가 높다

<b>장점</b>

- 모델이 보지 못한 작업에도 적응할 수 있는 범용성 보장
- 계산 효율성이 높음
- 특정 도메인에 특화된 모델로 조정하는데 사용할 수 있음
- RLHF와 같은 다른 정렬 기법과 병행하여 사용 가능

#### Limitations of Instruction Tuning
- 주석자의 편향이 개입될 가능성이 있음
- 고품질 데이터셋을 확보하는 데 많은 비용 요구
- 수십억 개 이상의 파라미터를 가진 모델을 학습하는 것 자체가 비용이 막대
- (일부 연구자 의견 기반) AI가 작성한 지시나 테스트 데이터는 일종의 증류 역할을 하나, 사람이 작성한 텍스트를 사용하는 것보다 효과가 떨어진다고 함.

## 3. 작고 효율적인 LLM 탐색하기
실제로, 산업에서 <u>1,000억 개 이상의 파라미터 모델이 반드시 필요한 것이 아닌 <b>특정 작업에 특화된 소규모 언어 모델 (SLM; Small Language Model)만으로도 충분한 경우</b>가 많다</u>.

LLM의 성능 (다양한 지식 통합, 맥락적 풍부함, 추론)에 비해 SLM이 범용성이 떨어질 수 있으나, <u>훨씬 적은 자원을 소비</u>하며 상용 GPU, CPU, 극단적인 경우 스마트폰 환경에서도 사용할 수 있다.

<b>은닉 차원</b>이 작은 모델은 이야기의 연속성을 유지하는 데 어려움을 겪는데, 은닉 차원이 128 이상은 되어야 연속성을 유지할 수 있음

주어진 입력에 따라 이야기를 이어가는 기능을 수행하기 위해서는 <u><b>적어도 두 개 이상의 레이어가 필요</b>하며, 레이어 수가 증가할수록 모델의 능력도 거의 비례하여 향상</u>된다. 단일 레이어 모델만으로는 충분한 전역 표현을 생성하지 못한다.

모델의 성능과 크기 사이에는 분명한 상충 관계가 존재한다.
#### 작고 효율적인 LLM을 얻는 방법

|방법|내용|
|-|-|
|소형 LLM을 처음부터 학습|Mistral 7B, LLaMA 7B|
|지식 증류|대형 모델을 활용해 소형 모델을 특정 작업에 맞춰 학습시키는 방식|
|모델 크기 축소|양자화, 가지치기 기법|

### 3.1. 양자화
<b>개념</b>: 모델 내 파라미터 표현 방식 자체를 줄이는 과정<br>
<b>핵심</b>: 고정밀 데이터 타입으로 표현된 가중치를 저정밀 데이터 타입으로 맵핑
<p align="center">
  <img src="https://github.com/user-attachments/assets/e2984d16-d6ac-4bf7-9a2b-3082b64ed482"><br>
  <b>Figure 7.</b> Quantization 과정 예시
</p>

<b>고려사항</b>: Float → Int는 정밀도를 감소시키는 작업 → 성능 저하 발생

#### 3.1.1. 어파인 양자화 매핑
<b>개념</b>: 고정밀 숫자를 저정밀 숫자로 변환할 때 두 가지 인자를 사용하여 값을 맵핑

어떤 값 $x$가 구간 [$\alpha, \beta$]에 존재할 때, 이 값을 양자화하면 $x_q \in [\alpha_q, \beta_q$] 형태를 얻을 수 있다.

$$
x_q=\rm{round}\big(\frac{1}{\it{s}} \it{x+z}\big)
$$
$$
s=\frac{\beta - \alpha}{\beta_q - \alpha_q} ~~~~ z=\rm{round}\big(\frac{\beta\alpha_q-\alpha\beta_q}{\beta-\alpha})
$$
- $\rm{round}()$ : 매핑 정확도 향상을 위함

$$
x_q = \rm{clip}\big(\rm{round}\big(\frac{1}{\it{s}} \it{x+z}\big), \alpha_q, \beta_q\big)
$$
- 양자화 직후, 타겟으로 하는 양자화 범위 (0~255)를 벗어나는 경우가 발생할 수 있음
- 교재 수식에 Clip 수행이 생략되어 있음

### 3.2. 가지치기
모델의 모든 파라미터가 유용한 것은 아니다. 많은 선형 종속성이 존재하고 모델이 사실상 과소적합 상태이기 때문에 불필요한 가중치가 다수 존재한다.

<b>개념</b>: 가중치나 연결, 레이어를 제거하는 작업

#### 3.2.1. 비구조적 가지치기
<b>개념</b>: 사전 학습된 모델에서 연결이나 개별 뉴런을 제거하여 파라미터를 0으로 만드는 기법 <br>
\- 특정 임계값보다 작은 값을 가진 연결을 0으로 설정하는 방식 <br>
\- 0에 가까운 가중치는 <u>이미 유의미한 정보를 거의 담고 있지 않으므로</u> <b>제거대상</b> <br>
\- 추론 시 최적의 성능을 내지 못하는 희소 모델이 될 수 있다

#### 3.2.2. 구조적 가지치기
<b>개념</b>: 개별 연결만 제거하는 대신 뉴런, 뉴런 그룹, 구조적 구성 요소, 전체 레이어나 블록까지 제거하는 기법 <br>
\- 정확도와 압축률 사이의 균형을 맞추면서 원래 모델의 성능을 최대한 유지하는 것

<p align="center">
  <img src="https://github.com/user-attachments/assets/ed2bc8a1-1919-494d-aad2-3123ff71068c"><br>
  <b>Figure 8.</b> 비구조적 가지치기, 구조적 가지치기의 예시 [8]
</p>

\- 고전적인 신경망에서 대부분의 가지치기 알고리즘은 손실 함수와 가중치의 곡률을 분석해 어떤 가중치가 중요한지 식별하는 방식을 사용 <br>
\- 이를 <b>최적 뇌 외과 알고리즘 (OBS; Optimal Brain Surgeon)</b>이라 부름 <br>
\- 또 다른 방법으로, 모델을 학습한 뒤 연결성을 줄이고 압축된 모델을 다시 학습하는 방식 → 이 과정을 여러 주기에 걸쳐 반복할 수 있다.

#### Limitations of Pruning
- 학습과 가지치기 주기를 반복하는 데 드는 비용이 너무 크기 때문에 수십억 개의 파라미터로 구성된 LLM에는 적용하기 어려움
- 가지치기 이후 모델을 파인튜닝하는 방법을 제안한 연구가 존재하나, 이 역시 LLM에서는 여전히 고비용을 요구

최근, 재학습이 필요 없는 가지치기 기법을 찾는데 있다.

지나치게 공격적인 가지치기는 종종 LLM 붕괴로 이어지며, 실제로 많은 알고리즘이 10% 이상의 가중치를 제거하면 모델 붕괴를 피하지 못한다.

```
모델 출력 = 입력 임베딩 + 각 레이어의 출력
```
이 때 일부 항은 큰 기여를 하지 않는데, 이러한 항들이 완전히 독립적이지 않아서 레이어를 제거하면 불일치가 발생할 수 있다.

- 각 레이어의 출력을 분석하면 어떤 레이어가 중요한지 파악할 수 있다.
- 레이어가 매우 깊은 모델에서 일부 레이어가 유사한 표현을 학습하기도 한다.
- 보통은 깊은 레이어일수록 초기 레이어보다 더 특화된 표현을 학습한다.

일부 연구에서 이러한 가정을 바탕으로 깊은 레이어에서 표현이 유사한 레이어를 제거하는 시도를 했으며, 그 결과 대규모 모델일수록 소규모 모델보다 중복된 레이어가 훨씬 많았고, 성능 저하 없이 효율적으로 압축할 수 있음이 밝혀졌다.

<b>Conclusion</b>: 메모리 사용량과 추론 시간을 줄여줄 뿐 아니라 모델 내부의 구조적 구성 요소의 중요성 연구에도 활용할 수 있고, 양자화 같은 다른 기법과 결합해 더 강력한 압축 효과를 얻을 수 있다.

**Paper**  Efficient Vision Transformers Optimized with Block Pruning and Quantization for HMD, <i>2025 IEEE International Symposium on Consumer Technology (ISCT)</i> [[**Github**](https://github.com/TF-polygon/Block-Pruning-and-Quantization-Sentis)][[**Paper**](https://doi.org/10.1109/ISCT66099.2025.11297378)]

## 4. 멀티모달 모델 탐색하기
**멀티모달 (Multimodal)**: 텍스트, 이미지, 음성, 비디오 등 서로 다른 형태의 데이터를 동시에 받아들이고 처리하는 기술

트랜스포머 블록은 벡터를 입력으로 하므로, <u>어떤 데이터 타입이든 **벡터로 변환한다면** 트랜스포머 블록의 입력으로 활용</u>할 수 있다.

**핵심**: 각 데이터 타입에 대해 잠재 표현을 얻는 방법을 찾는 것

### 4.1. Vision Transformer

<p align="center">
  <img src="https://github.com/user-attachments/assets/d472f819-56cf-4855-a890-27c97b6f4bf3"><br>
  <b>Figure 9.</b> Vision Transformer 구조 [9]
</p>

- Transformer Encoder만으로 구성
- 입력 이미지를 $16 \times 16$ 크기의 패치로 분할. 각 패치는 텍스트의 토큰과 유사하게 취급
- 단일 픽셀은 정보량이 부족하여 여러 픽셀을 묶는 패치 단위가 적절
- 분할된 패치를 평탄화 작업 수행 후 일련의 패치 시퀀스로 변환

**이미지는 다중 채널이 존재한다.**
$$
N=\frac{HW}{P^2}
$$
$H$는 이미지의 높이, $W$는 이미지의 너비, $C$는 채널 수, $P$는 패치 크기이며, $N$은 토큰 수이다.

선형화 이후 토큰의 길이는 $RGB:P^2 \times 3$, $BN:P^2 \times 1$

- 이 시점에서 **클래스를 나타내는 특수 토큰을 추가**하며, 모델이 **이미지 내 패치의 위치를 파악할 수 있는 위치 인코딩을 함께 추가**한다.

ViT는 *이미지 분류*, *객체 탐지*, *이미지 분할* 등 다양한 작업에 활용할 수 있다.

### 4.2. CLIP
LLM은 텍스트를 벡터로 표현하고, ViT는 이미지를 벡터로 표현한다. 각 모델은 특정 데이터 타입에 대해 단일 모드 임베딩을 생성하는 셈이다. 반면 멀티모달 임베딩은 이미지와 텍스트 정보를 동시에 포착하여 서로 연결할 수 있다.

**멀티모달 임베딩**: 이미지와 텍스트를 같은 표현 공간에 투영하여, 이전에는 불가능했던 작업에도 이 임베딩을 활용할 수 있다. <br>
\- 주어진 캡션 $x$가 있다면, 이 캡션과 의미적으로 유사한 이미지들을 검색하거나 반대로 특정 이미지와 연관된 캡션을 찾을 수 있다.

**CLIP (Constrastive Language Image Pre-training)**: 이미지와 텍스트 모두에 대해 임베딩을 생성하는 모델 (<U>사전 학습된 이미지 인코더와 텍스트 인코더를 결합</U>)

<p align="center">
  <img src="https://github.com/user-attachments/assets/bdac09ea-162d-412b-8b8f-f64d3bc70897"><br>
  <b>Figure 9.</b> CLIP 구조 [10]
</p>

- CLIP은 <U>최대한 많은 시각적 개념을 다루기 위해 인터넷에서 수집한 <B>4억 개의 이미지-텍스트 쌍 데이터셋으로 학습</B></U>함
- 두 데이터 타입 각각에 맞는 인코더를 사용하여 이미지와 해당 캡션에 대한 표현을 생성함
- 해당 인코더로 이미지와 캡션을 임베딩하면, 두 임베딩을 코사인 유사도로 비교함
- 모델은 이미지와 그에 대응하는 캡션 간의 코사인 유사도를 최대화하고, 동시에 잘못된 다른 짝과의 유사도는 최소화하는 방향으로 학습
- 텍스트 임베딩과 유사한 과정이나, 멀티모달이라는 점이 다르다.
- 이렇게 계산한 유사도를 기반으로 두 인코더의 파라미터를 업데이트
- 이러한 학습 방식을 **대조 학습**이라 함.

**CLIP의 학습은 올바른 짝을 예측하는 분류 작업으로 구성**되며, 모델의 예측과 실제 정답을 비교해 Cross Entropy Loss로 최적화한다.

### 4.3. VLM
CLIP은 이미지 캡션 생성처럼 텍스트 생성을 요구하는 작업에는 사용할 수 없으므로, <u>LLM처럼 동작하면서 이미지에 대한 질문에도 답할 수 있는 <b>VLM (Vision Language Model)</b>이 필요</u>하다.

#### 4.3.1. BLIP-2
<b>핵심</b>: 모델을 처음부터 새로 학습시키는 대신, 기존의 LLM과 ViT를 **Q-Former**라는 브리지 모듈로 연결 <br>

**Q-Former**: LLM에 시각을 부여하는 역할 (이미지 인코더와 LLM을 이어주는 중간 구성 요소)

<p align="center">
  <img src="https://github.com/user-attachments/assets/b6f5b747-d496-465a-aa0b-34a22b367ef1"><br>
  <b>Figure 10.</b> BLIP-2 프레임워크 [11]
</p>

|구성 요소|역할|
|-|-|
|Image Transformer|수많은 이미지 데이터 중에서 "텍스트와 연결될 만한 핵심적인 시각 정보"가 무엇인지 쿼리가 질문을 던지고 답변을 받아오는 과정으로, 이미지 인코더의 출력값과 Cross Attention을 수행|
|Text Transformer|텍스트 정보를 처리하면서 앞서 수행한 이미지 처리에서 발생한 시각 쿼리와 텍스트의 정렬을 시도|
1. 이미지-캡션 쌍을 활용해 Q-Former가 이미지와 텍스트 사이의 연관성을 학습
2. 학습된 Q-Former가 생성한 임베딩으로부터 소프트 프롬프트를 생성 <br>
\- 이 프롬프트는 LLM의 입력이 되어 LLM이 이미지의 내용을 이해하고 이에 맞춰 텍스트를 생성하도록 유도 <br>
\- Q-Former가 학습을 마치면, 이 모델을 활용해 이미지로부터 텍스트를 생성할 수 있다.

### 4.4. Text to Image
<b>개념</b>: 입력으로 주어진 텍스트의 정보를 기반으로 이미지를 생성

#### 4.4.1. Stable Diffusion
<b>개념</b>: 이미지 생성 품질, 성능, 대중적 접근성 측면에서 하나의 이정표로 평가받는 모델. <br>
<b>핵심</b>: U-Net을 사용한다는 것. 여기에서 Diffusion 과정이 이루어진다
|구성 요소|내용|
|-|-|
|텍스트 인코더|텍스트를 입력받아 벡터 표현으로 변환하는 모델|
|이미지 생성기|U-Net을 사용해 이미지 표현을 생성|
|이미지 디코더|이미지 표현을 실제 이미지로 변환|

#### U-Net
이미지를 직접 다루지 않고 <u>잠재 표현이라는 압축된 표현을 사용</u>한다. 이 잠재 표현에는 <u>이미지를 생성하는데 필요한 정보가 담겨 있으며, 마지막 단계에서 <b>디코더가 이를 바탕으로 최종 이미지를 생성</b></u>한다.

확산 과정에서 모델은 <u>무작위 노이즈에서 시작해 <b>점차 이미지 정보를 담은 잠재 표현을 구축</b>해 나간다</u>. 확산 모델의 핵심 아이디어는 충분히 큰 학습 데이터셋이 주어지면, 그 안에 내재된 패턴을 모델이 학습할 수 있다는 점이다.

**학습 과정**
- 실제 이미지 하나를 선택
- 무작위 노이즈를 생성
- 일정량의 노이즈를 이미지에 추가

위 과정을 통해 <u>노이즈 양을 조절하여 <b>원본 이미지의 다양한 변형을 만들 수 있고</b>, 그 결과 이미지 데이터셋을 대폭 확장</u>할 수 있다.

이후 모델은 <u>추가된 노이즈를 식별하고 제거할 수 있도록 학습</u>된다. 이 과정은 <b>고전적인 역전파를 통해 이루어진다.</b>

<p align="center">
  <img src="https://github.com/user-attachments/assets/f94b3af2-9185-4fc5-ba7b-db7770a7f339"><br>
  <b>Figure 11.</b> Stable Diffusion Architecture [12]
</p>

- <u>이미지를 직접 예측하는 대신, <b>이미지를 얻기 위해 제거해야 할 노이즈의 분포를 예측</u></b>
- 노이즈 제거 과정을 수행하면서 <u>역방향 이미지 또는 최소한 그 잠재 표현을 복원</u>
- 위 과정으로, 모델은 무작위 노이즈에서 시작해 점차 이미지를 만들어내는 법을 학습하며, 노이즈 속에서 이미지를 찾아내는 능력을 얻게 된다.

이 단계가 끝나면 디코더를 통해 실제 이미지를 생성할 수 있다. 다만, 이 시점까지는 <b>텍스트를 활용해 이미지 생성을 제어할 수는 없다</b>.

- LLM의 성능이 좋을수록 모델이 가져올 수 있는 정보의 질 역시 높아지기 때문에 <U><B>텍스트 인코더에 어떤 LLM을 사용하는지 매우 중요</B>하다.</U>

## 5. 할루시네이션과 윤리적/법적 쟁점 이해하기

**할루시네이션**: 무의미하거나 신뢰할 수 없는 내용을 생성하는 현상
- 사실적 환각: <u>실제로 검증 가능한 사실과 모순되는 응답</u>을 생성하는 경우
- 충실도 환각: <u>사용자 지시나 주어진 맥락과 어긋나는 내용</u>을 만들어내는 경우

모델은 일관된 텍스트를 생성하도록 학습되지만, 스스로 출력을 교정하거나 정확성을 확인할 수단은 없다.

### 5.1. LLM의 사용과 관련한 잠재적 피해
**표현적 피해**: 모델이 편향이나 고정관념을 강화할 때 발생한다. 감정 분류기가 특정 짇반에 더 낮은 감정 점수나 부정적인 감정을 할당했던 사례가 보고된 바 있다.

**할당적 피해**: 모델이 기회, 자원, 서비스 등을 불공평하게 분배할 때 발생한다. <br>
\- 소수 집단을 표현할 때 공격적이거나 비하적인 언어를 사용할 수 있으며, 문화적 규범, 태도, 편견에 대한 사회적 고정관념을 그대로 재생산할 수 있다.

ex) LLM이 의료 서비스 (혹은 채용이나 대출)의 우선순위를 결정하는 데 활용될 경우, 학습 과정에서 내재된 편향 때문에 불공정한 결과를 낳을 수 있다.

### 5.2. 편향 문제
<u>임베딩 모델이 편향을 증폭시키고 <b>이 편향이 임베딩 공간에 반영</b>된다는 점</u>은 여러 차례 보고되고 있음 <br>
\- 실제로 유해한 콘텐츠가 특정 집단이나 소수 집단과 연결되는 현상도 임베딩 공간에서 확인됨.

**편향의 원인**: 사전 학습 데이터에서 비롯됨<br>
- 학습 이전에 **데이터 정화를 거쳐 유해한 콘텐츠를 제거**해야 한다. <br>
- 파인튜닝 할 때, <u>레이블을 부여하는 <b>주석자의 편향으로 인해 잘못된 레이블이 포함되지는 않았는지</b> 반드시 점검</u>해야 한다. <br>
- 학습 데이터의 출처를 다양화 하는 것도 중요하다

\- 실제로 모델 학습에 사용되는 데이터는 미국에서 생상된 텍스트와 다른 국가에서 생산된 텍스트 간에 큰 불균형을 보인다. <br>
\- 이로 인해, 사전 학습 과정에서 지배적인 인구 집단의 시각을 그대로 물려받게 된다.

### 5.3. 허위 정보 문제
LLM은 신뢰할 수 있고 설득력 있는 텍스트를 생성할 수 있기 때문에, 악의적인 행위자들은 이를 활용해 허위 정보, 피싱 이메일 등 <U>유해한 콘텐츠를 자동으로 대량 생산이 가능하다.</U>

<p align="center">
  <img src="https://github.com/user-attachments/assets/12f652cd-a430-40b7-b306-535b2dbc921d"><br>
  <b>Figure 12.</b> LLM이 생성하는 허위 정보 분류 체계 [13]
</p>

모델의 편향을 연구할 수 있도록 돕는 여러 데이터셋과 파이썬 라이브러리가 존재
- Hugging Face의 Evaluate 라이브러리
```python
import evaluate

toxicity = evaluate.load("toxicity")
toxicity.compute(
  predictions=model_continuations,
  aggregation="ratio,
)
```
```python
regard = evaluate.load("regard", "compare")
regard_results = regard.compute(
  data = profession1_completions,
  references = profession2_completions,
)
```

### 5.4. 저작권 문제
- LLM은 저작권이 있는 텍스트를 학습에 사용
- 학습에 활용된 텍스트 일부를 재생성
- 지금까지 LLM의 개발자들은 이러한 학습 행위가 <B><U>공정 사용</B> 원칙에 해당한다고 주장</U>
- 현재는 정치적, 법적 환경을 뒤흔들 수 있는 여러 소송이 진행 중
-일부 기업들은 이에 대응해 신문사나 소셜 네트워크 플랫폼과 라이선스 계약을 체결하려는 움직임을 보임

### 5.5. 개인정보 침해 위험 문제
- 모델은 학습 데이터에 포함된 <u>정보를 그대로 노출할 수 있으며</u>, 적대적 공격을 통해 특정 정보를 추출하는 것도 가능
- 개인 정보가 포함된 데이터베이스로 학습할 경우, <u>해당 정보가 외부로 유출될 가능성이 있음</u>

모델이 개인 데이터를 잊게 만드는 **머신 언러닝** 기법이 연구되고 있다.

\- 실제로 사용자가 요청할 경우 모델이 해당 정보를 삭제하도록 요구하는 법률 제정을 일부 국가에서 추진 중

## 6. 프롬프트 엔지니어링
### 6.1. 인-컨텍스트 러닝
**핵심**: 모델을 업데이트 하지 않고, 프롬프트의 입출력으로 새로운 작업을 수행할 수 있다.

- LLM의 입출력 쌍 $(x, y)$가 주어졌을 때, 모델이 입력 $x$와 출력 $y$ 사이의 관계를 학습
- 모델이 풀어야 하는 문제가 학습 데이터에서 풀었던 문제와 유형이 조금이라도 달라지면 성능은 급격히 저하

\- ICL을 활용할 시, 모델 자체를 <u><b>학습으로 업데이트 하지 않고도 새로운 작업에 적용하여</b> 위와 같은 성능 저하 문제를 해결</u>할 수 있다.<br>
\- 다만 내부 파라미터가 전혀 업데이트되지 않으므로 <b>프롬프트로부터 얻은 일시적인 능력</b>이다. <br>
\- 대신 모델은 사전 학습을 통해 이미 습득한 잠재 표현을 활용해 새로운 작업 수행<br>
\- 프롬프트에 주어진 입력과 출력 간의 관계를 바탕으로, 모델은 사전 학습 과정에서 배운 지식을 불러와 적절한 잠재 함수를 찾아낸다.

<b>장점</b><br>
- 인간의 인지적 추론 과정을 모방
- 파라미터 업데이트가 필요 없다
- 다양한 벤치마크에서 경쟁력 있는 성능을 보여준다.

하지만, 이러한 동작이 어떻게 발현되는지는 완전히 규명되지는 않음...

### 6.2. 프롬프트의 요소 및 용어 정의

프롬프트는 일반적으로 입력 형식, 입력값, 출력값, 그리고 입력-출력 매핑과 같은 다양한 요소를 제공하며, 이러한 **요소들이 모델이 올바른 매핑을 성공적으로 수행하는 데 중요한 역할**을 한다.

모델은 프롬프트에 포함된 예시를 단서로 삼아 주어진 작업이 무엇인지를 추론할 수 있으며, 프롬프트의 다른 요소들은 모델이 자신의 파라미터 속에서 필요한 잠재 개념을 찾아내도록 돕는다.

<p align="center">
  <img src="https://github.com/user-attachments/assets/4a0d5792-512a-42c6-88a0-9c198fb803fb"><br>
  <b>Figure 13.</b> 프롬프트 구조 [14]
</p>

보통 프롬프트에는 질문과 지시문이 포함된다.

- **제로샷 프롬프팅**: 예시 없이 지시만 있는 경우
```
When was Shakespeare born?
```
- 위와 같이 <u>예시 없이 지시만 내리는 유형의 프롬프트에 성공적으로 응답하는 모델은 **제로샷 능력을 가졌다**</u>고 함
- 현재 대부분의 LLM이 채택하고 있는 두 가지 학습 프로세스에서 비롯된다.
  - 사전 학습 과정
  - 지시 튜닝

<br>

- **퓨샷 프롬프팅**: 예시 제공

```
This movie is awesome - positive
This andwich is disgusting - negative
This TV series is meh -
```
프롬프트에는 모델을 돕기 위한 컨텍스트나 원하는 응답 형식을 추가할 수도 있다.

- **사고의 사슬 프롬프팅**: <입력, 사고의 사슬, 출력>의 세 가지 요소 제공

제로샷, 퓨샷 프롬프팅과 같은 **단순 프롬프트 방식**은 <u>추론이 필요한 작업에서 한계를 보인다</u>. 특히, 다단계 추론을 요구하는 작업에서는 <u>단순히 예시만 제공해서는 **모델을 올바른 방향으로 유도하기 어렵다**</u>.

**사고의 사슬 프롬포팅** 중 **사고의 사슬**은 문제를 해결하기 위한 여러 중간 단계를 의미

<p align="center">
  <img src="https://github.com/user-attachments/assets/0e492658-d167-406b-9f13-7bcc63108381"><br>
  <b>Figure 14.</b> CoT 예시 [15]
</p>

- 다양한 문제에 대해 <u>양질의 시연을 확보해야 한다는 단점 존재</u>
- 주석이 달린 데이터셋을 수집하는 과정도 많은 비용 요구

**장점**: 과제를 작은 단계들로 분해해 모델이 다루기 쉽게 만든다는 점

프롬프트에 `Let's think step be step`라는 문구를 덧붙이는 것만으로도 이런 거동을 유도할 수 있는데, 이를 **제로샷 CoT 프롬프팅**이라 한다.

- **자기 일관성**: 앙상블의 개념을 기반으로 모델의 추론 능력 강화를 위해 활용
  - 하나의 문제에 대해 여러 해답을 생성한 뒤, 다수 의견에 해당하는 해답을 선택하는 방식

- **사고의 나무**: 모델의 추론과 자기 평가 능력을 활용하는 방식
  - 모델이 여러 사고의 중간 단계를 생성
  - 여기에 탐색 알고리즘 (DFS, BFS)를 적용하여 각 후보 경로를 평가
    - 이 과정에서는 일반적으로 후보 경로와 단계 수를 사전에 선택해야 함
  - 여러 응답을 생성 해야 하므로 그만큼 계산 비용이 증가한다는 단점 존재

**DSPy**<BR>
**개념**: 프롬프트를 시그니처라는 형태로 추상화하고 이를 파인튜닝하며, 다양한 프롬프트 기법들을 모듈로 활용할 수 있다. <BR>
\- 이렇게 하면, 프롬프트 엔지니어링은 옵티마이저를 활용해 자동화 할 수 있다.

1. 데이터셋이 주어지면 시그니처와 모듈을 포함한 DSPy 파이프라인 구성
2. 최적화할 지표를 정의
3. 목표 출력과 옵티마이저를 정의
4. 최적화 실행
5. 반복
6. 최적의 프롬프트 탐색



## Reference
[1] Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361 (2020).<br>
[2] 주인이 없는 의료 데이터가 주목받는 이유, brunch, [online] https://brunch.co.kr/@3b8bf521dd714f7/6 (Accessed 20th Jan)<br>
[3] Wei, Jason, et al. "Emergent abilities of large language models." arXiv preprint arXiv:2206.07682 (2022).<br>
[4] Fedus, William, Barret Zoph, and Noam Shazeer. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." Journal of Machine Learning Research 23.120 (2022): 1-39.<br>
[5] Liu, Yang, et al. "Trustworthy llms: a survey and guideline for evaluating large language models' alignment." arXiv preprint arXiv:2308.05374 (2023).<br>
[6] Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." Advances in neural information processing systems 36 (2023): 53728-53741.<br>
[7] Lee, Geon-Hee, et al. "Efficient Vision Transformers Optimized with Pruning and Quantization for HMD." 2025 IEEE International Symposium on Consumer Technology (ISCT). IEEE, 2025.<br>
[8] Chen, Liyang, et al. "Knowledge from the original network: restore a better pruned network with knowledge distillation." Complex & Intelligent Systems 8.2 (2022): 709-718.<br>
[9] Dosovitskiy, Alexey. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020). <br>
[10] Radford, Alec, et al. "Learning transferable visual models from natural language supervision." International conference on machine learning. PmLR, 2021.<br> 
[11] Li, Junnan, et al. "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models." International conference on machine learning. PMLR, 2023.<br>
[12] Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.<br>
[13] Chen, Canyu, and Kai Shu. "Can llm-generated misinformation be detected?." arXiv preprint arXiv:2309.13788 (2023).<br>
[14] Work, What Makes In-Context Learning. "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?."<br>
[15] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.<br>